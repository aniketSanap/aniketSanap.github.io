<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="/assets/images/logo.png">

<title>Understanding RNNs | Aniket Sanap</title>

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Understanding RNNs | Aniket Sanap</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Understanding RNNs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Understand the working of this powerful deep learning architecture" />
<meta property="og:description" content="Understand the working of this powerful deep learning architecture" />
<link rel="canonical" href="http://localhost:4000/Understanding-RNNs/" />
<meta property="og:url" content="http://localhost:4000/Understanding-RNNs/" />
<meta property="og:site_name" content="Aniket Sanap" />
<meta property="og:image" content="http://localhost:4000/assets/images/posts/RNNs/RNN.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-18T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/assets/images/posts/RNNs/RNN.png" />
<meta property="twitter:title" content="Understanding RNNs" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/Understanding-RNNs/"},"description":"Understand the working of this powerful deep learning architecture","url":"http://localhost:4000/Understanding-RNNs/","@type":"BlogPosting","image":"http://localhost:4000/assets/images/posts/RNNs/RNN.png","headline":"Understanding RNNs","dateModified":"2020-02-18T00:00:00-05:00","datePublished":"2020-02-18T00:00:00-05:00","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"}},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="/assets/css/screen.css" rel="stylesheet">

<link href="/assets/css/main.css" rel="stylesheet">

<link href="https://fonts.googleapis.com/css?family=Raleway&display=swap" rel="stylesheet">

<script src="/assets/js/jquery.min.js"></script>

</head>




<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="/">
    <!-- <img src="/assets/images/logo.png" alt="Aniket Sanap"> -->
    <!-- <p style=''>Aniket Sanap</p> -->
    <span style="font-family: 'Raleway', sans-serif; font-size: larger;"><b>Aniket Sanap</b></span>
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav ml-auto">

                
                <li class="nav-item">
                
                <a class="nav-link" href="/index.html">Blogs</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="/about">About</a>
                </li>

                <!-- <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://bootstrapstarter.com/bootstrap-templates/template-mediumish-bootstrap-jekyll/"> Docs</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-wordpress/"><i class="fab fa-wordpress-simple"></i> WP Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-ghost/"><i class="fab fa-snapchat-ghost"></i> Ghost Version</a>
                </li> -->


                <li class="nav-item">
                    <a target="_blank" class="nav-link" href="https://github.com/aniketsanap"><i class="fab fa-github"></i></a>
                </li>

                <li class="nav-item">
                    <a target="_blank" class="nav-link" href="https://www.linkedin.com/in/aniket-sanap-940145185/"><i class="fab fa-linkedin"></i></a>
                </li>
                
                <li class="nav-item">
                    <a target="_blank" class="nav-link" href="https://instagram.com/aniketv2/"><i class="fab fa-instagram"></i></a>
                </li>

                <li class="nav-item">
                    <a target="_blank" class="nav-link" href="mailto:aniket.s2509@gmail.com"><i class="far fa-envelope"></i></a>
                </li>


                <script src="/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="/assets/js/lunrsearchengine.js"></script>

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->

<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<!-- <div class="mainheading">
    <h1 class="sitetitle">Aniket Sanap</h1>
    <p class="lead">
        Machine learning enthusiast.
    </p>
</div> -->

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-2 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=Understanding RNNs&url=http://localhost:4000/Understanding-RNNs/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=http://localhost:4000/Understanding-RNNs/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/Understanding-RNNs/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
</div>

        </div>

        <!-- Post -->
        

        <div class="col-md-9 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Author Box -->
                

                <!-- Post Title -->
                <h1 class="posttitle">Understanding RNNs</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            

            
            <img class="featured-image img-fluid" src="/assets/images/posts/RNNs/RNN.png" alt="Understanding RNNs">
            

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                <!-- End Toc -->
                <script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2>

<p>Linear layers and convolutional layers are very popular in neural network architectures. So what is the need of Recurrent Neural Networks? There is an obvious limitation to the former architectures. Can you guess what it is?<br />
.<br />
.<br />
.<br />
.<br />
<strong>Memory!</strong><br />
These architectures don’t store any information about the previous inputs given to the network. This mean they tend to give poor results while working with sequential data (for the most part). Humans don’t start thinking from scratch at every instant. Just while reading this sentence, you have an idea of the words which came before and the ones to follow. A linear model processes each input independently. So you must convert the entire sequence into one input data point. Hence, they are stateless.</p>

<h3 id="what-is-an-rnn">What is an RNN?</h3>

<p>An RNN is an architecture which unlike Linear models, preserve state. They process sequences by iterating through its elements and maintaining a <b>state</b>. This state is reset while processing two different sequences. This is what a simple RNN looks like:</p>

<p><img src="/assets/images/posts/RNNs/RNN.png" /></p>

<p>The saved state is called the <b>hidden state</b>. An RNN processes each element of the sequence sequentially. At each time step, it updates its hidden state and produces an output. This is what happens when we <strong>unroll</strong> an RNN:</p>

<p><img src="/assets/images/posts/RNNs/RNN_unrolled.png" /></p>

<p>Unrolling an RNN is simply visualizing how it processes the sequence element by element. In reality, the RNN consists of just one cell processing the input in a loop. This property of an RNN allows it to process variable length inputs. RNNs are just a <strong>refactored, fully-connected neural network.</strong></p>

<p>The working of an RNN (at timestep \(t\)) is as follows:
An RNN consists of 3 weight matrices: \(W_x\), \(W_h\), \(W_y\).</p>
<ul>
  <li>\(W_x\) is the weight matrix for the input (x).</li>
  <li>\(W_h\) is the weight matrix for the hidden state.</li>
  <li>\(W_y\) is the weight matrix for the output.</li>
</ul>

<p>The hidden state is given by:<br />
\(H_t = \sigma(W_x * X_t + W_h * H_{t-1})\)</p>
<ul>
  <li>\(H_t\) is the hidden state at timestep \(t\).</li>
  <li>\(\sigma\) is the activation function (generally sigmoid or tanh).</li>
  <li>\(X_t\) is the input at the current timestep.</li>
</ul>

<p>The output of the RNN is given by:<br />
\(y = \sigma_y(W_y * H_t)\)</p>

<p>Due to the sequential nature of natural language, RNNs are commonly used in Natural Language Processing. 
Let us try to better understand the working of RNNs using an example.</p>

<p>In this example we are going to build a model to classify names into two countries of origin -&gt; Italy and Germany. Our dataset consists of two files <code class="language-plaintext highlighter-rouge">Italian.txt</code> and <code class="language-plaintext highlighter-rouge">German.txt</code>. Both of these files contain a single name on each line. 
The data can be downloaded by:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>wget https://download.pytorch.org/tutorial/data.zip
</pre></td></tr></tbody></table></code></pre></div></div>
<p>After unzipping the downloaded file, you will find several files which follow the following format:</p>
<pre><code class="language-plain">name_1
name_2
name_3
...
</code></pre>
<h2 id="cleaning-the-data">Cleaning the Data</h2>

<p>We start off by importing all the modules we will be using for this project. The only module you will need to install is of course <code class="language-plaintext highlighter-rouge">pytorch</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">random_split</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">string</span> <span class="kn">import</span> <span class="n">ascii_letters</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="reading-the-data">Reading the data</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'Projects/NameClassifier/data/names/German.txt'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">german_f</span><span class="p">,</span> <span class="nb">open</span><span class="p">(</span><span class="s">'Projects/NameClassifier/data/names/Italian.txt'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">italian_f</span><span class="p">:</span>
    <span class="n">german_names</span> <span class="o">=</span> <span class="n">german_f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
    <span class="n">italian_names</span> <span class="o">=</span> <span class="n">italian_f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'German names:</span><span class="se">\n</span><span class="si">{</span><span class="n">german_names</span><span class="p">[:</span><span class="mi">30</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Italian names:</span><span class="se">\n</span><span class="si">{</span><span class="n">italian_names</span><span class="p">[:</span><span class="mi">33</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Output:</p>
<pre><code class="language-plain">German names:
Abbing
Abel
Abeln
Abt
Achilles

Italian names:
Abandonato
Abatangelo
Abatantuono
</code></pre>

<h3 id="finding-all-the-unique-characters-in-the-files">Finding all the unique characters in the files</h3>

<p>The classifier which we are going to build is going to be character based. This means that it will take a sequence of characters as its input. Each name will be read by the model character by character. For this we need to first find all the unique characters in the files. We find all the unique characters in the files and then take its union with all letters (uppercase and lowercase) to form our vocabulary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="n">unique_characters</span> <span class="o">=</span> <span class="nb">set</span><span class="p">((</span><span class="n">german_names</span> <span class="o">+</span> <span class="n">italian_names</span><span class="p">).</span><span class="n">replace</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">,</span> <span class="s">''</span><span class="p">)).</span><span class="n">union</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">ascii_letters</span><span class="p">))</span>
<span class="n">unique_characters</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">unique_characters</span><span class="p">)</span>
<span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">unique_characters</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Output:</p>
<pre><code class="language-plain">" 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzßàäèéìòóöùü"
</code></pre>
<p>Doing this ensures that our vocabulary will contain as many used characters as possible. You can probably get away with just using <code class="language-plaintext highlighter-rouge">ascii_letters</code> to form your vocabulary. This is what our list of names looks like:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="n">german_names</span> <span class="o">=</span> <span class="n">german_names</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="n">italian_names</span> <span class="o">=</span> <span class="n">italian_names</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">german_names</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">italian_names</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Output:</p>
<pre><code class="language-plain">['Abbing', 'Abel', 'Abeln', 'Abt', 'Achilles']
['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']
</code></pre>
<h3 id="removing-common-names">Removing common names</h3>

<p>We don’t want our classifier to accept the same input with two different classes. Hence, we will find the names which exist in both the german and italian datasets and remove them from both</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">common_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">german_names</span><span class="p">).</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">italian_names</span><span class="p">)))</span>
<span class="n">common_names</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Output:</p>
<pre><code class="language-plain">['', 'Salomon', 'Paternoster']
</code></pre>

<p>After removing the common names:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="k">for</span> <span class="n">common_name</span> <span class="ow">in</span> <span class="n">common_names</span><span class="p">:</span>
    <span class="n">german_names</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">common_name</span><span class="p">)</span>
    <span class="n">italian_names</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">common_name</span><span class="p">)</span>
    
<span class="n">common_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">german_names</span><span class="p">).</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">italian_names</span><span class="p">)))</span>
<span class="n">common_names</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Output:</p>
<pre><code class="language-plain">[]
</code></pre>

<h3 id="creating-our-data">Creating our data</h3>

<p>We will create a list of all our names. This will be the input passed to our model. Along with this we will also need labels. We will have a label of <code class="language-plaintext highlighter-rouge">0</code> for german names and a label of <code class="language-plaintext highlighter-rouge">1</code> for italian names.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">german_label</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">italian_label</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">all_names</span> <span class="o">=</span> <span class="n">german_names</span> <span class="o">+</span> <span class="n">italian_names</span>
<span class="n">all_labels</span> <span class="o">=</span> <span class="n">german_label</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">german_names</span><span class="p">)</span> <span class="o">+</span> <span class="n">italian_label</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">italian_names</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">all_names</span><span class="p">[</span><span class="mi">720</span><span class="p">:</span><span class="mi">726</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">all_labels</span><span class="p">[</span><span class="mi">720</span><span class="p">:</span><span class="mi">726</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Output:</p>
<pre><code class="language-plain">['Zimmerman', 'Zimmermann', 'Abandonato', 'Abatangelo', 'Abatantuono', 'Abate']
[0, 0, 1, 1, 1, 1]
</code></pre>
<h3 id="one-hot-encoding-characters">One hot encoding characters</h3>

<p>For our model to be able to process our input, we have to convert the characters to one hot encoded vectors. The size of our vector will be the total number of unique characters in our dataset. Hence we will first create a mapping of our character and its index. We can then use this mapping to convert our input characters to digits.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">stoi</span> <span class="o">=</span>  <span class="p">{</span><span class="n">char</span><span class="p">:</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">char</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">unique_characters</span><span class="p">))}</span>
<span class="n">stoi</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Output:</p>
<pre><code class="language-plain">{' ': 0,"'": 1,'A': 2,'B': 3,'C': 4,'D': 5,'E': 6,'F': 7,'G': 8,'H': 9,'I': 10,'J': 11,'K': 12,'L': 13,'M': 14,'N': 15,'O': 16,'P': 17,'Q': 18,'R': 19,'S': 20,'T': 21,'U': 22,'V': 23,'W': 24,'X': 25,'Y': 26,'Z': 27,'a': 28,'b': 29,'c': 30,'d': 31,'e': 32,'f': 33,'g': 34,'h': 35,'i': 36,'j': 37,'k': 38,'l': 39,'m': 40,'n': 41,'o': 42,'p': 43,'q': 44,'r': 45,'s': 46,'t': 47,'u': 48,'v': 49,'w': 50,'x': 51,'y': 52,'z': 53,'ß': 54,'à': 55,'ä': 56,'è': 57,'é': 58,'ì': 59,'ò': 60,'ó': 61,'ö': 62,'ù': 63,'ü': 64}
</code></pre>

<p>While our RNN can accept inputs of variable length, we still have to define a sequence length. This will allow us to batch our data for parallel execution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">one_hot_encoder</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">stoi</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Size of stoi: </span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="c1"># To save output
</span>    <span class="n">encoded</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Iterating through name
</span>    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="c1"># Setting index of character to 1
</span>        <span class="n">temp</span><span class="p">[</span><span class="n">stoi</span><span class="p">[</span><span class="n">char</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">encoded</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span>
        
    <span class="c1"># Filling the rest of the sequence with zeros
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequence_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">name</span><span class="p">)):</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="n">encoded</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>

<span class="n">one_hot_encoder</span><span class="p">(</span><span class="s">'Aniket'</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<pre><code class="language-plain">Size of stoi: 65
tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       ...
</code></pre>

<h3 id="creating-our-dataset-object">Creating our dataset object</h3>

<p>Now we have done a lot of preprocessing! Let us combine all of this in our dataset class. We will set the sequence length to \(18\) as that is the length of the longest name in our dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">NameDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">german_fname</span><span class="o">=</span><span class="s">'Projects/NameClassifier/data/names/German.txt'</span><span class="p">,</span> <span class="n">italian_fname</span><span class="o">=</span><span class="s">'Projects/NameClassifier/data/names/Italian.txt'</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># Reading from files
</span>        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">german_fname</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">german_f</span><span class="p">,</span> <span class="nb">open</span><span class="p">(</span><span class="n">italian_fname</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">italian_f</span><span class="p">:</span>
            <span class="n">german_names</span> <span class="o">=</span> <span class="n">german_f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
            <span class="n">italian_names</span> <span class="o">=</span> <span class="n">italian_f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
        
        <span class="c1"># Finding unique characters
</span>        <span class="n">unique_characters</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">((</span><span class="n">german_names</span> <span class="o">+</span> <span class="n">italian_names</span><span class="p">).</span><span class="n">replace</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">,</span> <span class="s">''</span><span class="p">)).</span><span class="n">union</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">ascii_letters</span><span class="p">)))</span>
        <span class="n">german_names</span> <span class="o">=</span> <span class="n">german_names</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
        <span class="n">italian_names</span> <span class="o">=</span> <span class="n">italian_names</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
        
        <span class="c1"># Removing common names
</span>        <span class="n">common_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">german_names</span><span class="p">).</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">italian_names</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">common_name</span> <span class="ow">in</span> <span class="n">common_names</span><span class="p">:</span>
            <span class="n">german_names</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">common_name</span><span class="p">)</span>
            <span class="n">italian_names</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">common_name</span><span class="p">)</span>
        <span class="n">german_label</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">italian_label</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Setting names and labels
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">names</span> <span class="o">=</span> <span class="n">german_names</span> <span class="o">+</span> <span class="n">italian_names</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">german_label</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">german_names</span><span class="p">)</span> <span class="o">+</span> <span class="n">italian_label</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">italian_names</span><span class="p">)</span>
        
        <span class="c1"># Mapping from chars to int
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">stoi</span> <span class="o">=</span>  <span class="p">{</span><span class="n">char</span><span class="p">:</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">char</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">unique_characters</span><span class="p">))}</span>
        
        <span class="c1"># Size of longest word is 18
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">18</span>
        
        <span class="c1"># One hot encoded names
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoded_names</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encode_dataset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">one_hot_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">stoi</span><span class="p">)</span>

        <span class="n">encoded</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">temp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
            <span class="n">temp</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">stoi</span><span class="p">[</span><span class="n">char</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">encoded</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">sequence_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">name</span><span class="p">)):</span>
            <span class="n">temp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
            <span class="n">encoded</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">encode_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">encoded_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">names</span><span class="p">:</span>
            <span class="n">encoded_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">one_hot_encoder</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
            
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">encoded_list</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">labels</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoded_names</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Let us see what our output looks like:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">names</span> <span class="o">=</span> <span class="n">NameDataset</span><span class="p">()</span>
<span class="n">names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Output:</p>
<pre><code class="language-plain">(tensor([[0., 0., 1.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([0]))
</code></pre>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c1"># Shape of input tensor (one word)
</span><span class="n">names</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Output:</p>
<pre><code class="language-plain">torch.Size([18, 65])
</code></pre>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="n">device</span> <span class="o">=</span> <span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span>
<span class="n">split_ratio</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">data_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">split_ratio</span> <span class="o">*</span> <span class="n">data_len</span><span class="p">)</span>
<span class="n">test_size</span> <span class="o">=</span> <span class="n">data_len</span> <span class="o">-</span> <span class="n">train_size</span>

<span class="c1"># Randomly splits data into given sizes
</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">random_split</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">test_size</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="comparison-with-a-linear-model">Comparison with a Linear model</h3>

<p>Before we build our RNN based model, let us look at the results with a conventional Linear model. On our problem statement, using a model with just 3 linear layers, I was able to achieve an accuracy of just 69.2% even after training for 100 epochs. This problem is very simple with very short sequences. For a problem with longer sequences, the model’s performance would be even worse.</p>

<h3 id="building-an-rnn-using-linear-layers">Building an RNN using linear layers</h3>

<p>Let us revisit the mathematics behind an RNN:
\(H_t = \sigma(W_x * X_t + W_h * H_{t-1})\)
\(y = \sigma_y(W_y * H_t)\)</p>

<p>Where \(H_t\) is the hidden state at timestep \(t\) and \(y\) is the output of the RNN. For illustration purposes we will assume the batch size to be \(1\). Let us discuss some of the terminology commonly used while talking about RNNs.</p>

<h5 id="sequence-length">Sequence length</h5>

<p>Although RNNs can handle inputs of varying sequence lengths, tensor operations require the every sequence of the same size. This is why while RNNs can accept sequences of any length, for parallel processing we will maintain the same sequence length for every input batch. The sequence length for our model is \(18\) because that is the length of the longest name in our dataset.</p>

<h5 id="input-size">Input size</h5>

<p>This parameter of our RNN is different from the sequence length. This parameter signifies the size of one element of the sequence. For our example, the input size is \(65\) which is the size of our one hot encoded vector.</p>

<hr />

<p>Now let us try building our own RNN with just linear layers. We will use <code class="language-plaintext highlighter-rouge">nn.Linear</code> for  a linear layer. We need a linear layer for \(W_x, W_h\) and \(W_y\) each. We will also initialize the hidden state with <code class="language-plaintext highlighter-rouge">torch.zeros</code>. We will have a <code class="language-plaintext highlighter-rouge">for</code> loop to iterate through the sequence.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">LinearRNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">global</span> <span class="n">device</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">256</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">18</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="mi">65</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">Wx</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">input_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">Wh</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">Wy</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">sequence_length</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># input_tensor.shape[1] = sequence length
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>     
            <span class="c1"># input_tensor[:, i] = the ith element in the sequence
</span>            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">Wx</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]))</span>     
            <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">Wh</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="n">res</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span>        
        <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">sequence_length</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">Wy</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>
    
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Now let us create our <code class="language-plaintext highlighter-rouge">DataLoader</code> and set some hyperparameters before training the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">linear_train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">linear_test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRNN</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">5e-6</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">max_accuracy</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">MODEL_PATH</span> <span class="o">=</span> <span class="s">''</span>
<span class="k">if</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">MODEL_PATH</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Existing model found!'</span><span class="p">)</span>
    <span class="n">load_weights</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">MODEL_PATH</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'No existing model found.'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>This is what our model looks like:</p>

<pre><code class="language-plain">No existing model found.
LinearRNN(
    (Wx): Linear(in_features=65, out_features=256, bias=True)
    (Wh): Linear(in_features=256, out_features=256, bias=True)
    (Wy): Linear(in_features=4608, out_features=256, bias=True)
    (output_layer): Linear(in_features=256, out_features=2, bias=True)
)

</code></pre>

<p>Now it time to finally train the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre></td><td class="rouge-code"><pre><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">linear_train_loader</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span> <span class="k">continue</span>
        <span class="n">model</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_batch</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nb">long</span><span class="p">().</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)).</span><span class="n">item</span><span class="p">()</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Accuracy: </span><span class="si">{</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span> <span class="o">*</span> <span class="mi">100</span><span class="si">}</span><span class="se">\n</span><span class="s">Loss: </span><span class="si">{</span><span class="n">epoch_loss</span><span class="o">/</span><span class="n">total</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">test_epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">linear_test_loader</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span> <span class="k">continue</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_batch</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nb">long</span><span class="p">().</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>
                <span class="n">test_epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)).</span><span class="n">item</span><span class="p">()</span>

        <span class="n">test_accuracy</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'''### TESTING ###
        Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">}</span><span class="s">
        Loss: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">test_epoch_loss</span><span class="o">/</span><span class="n">total</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s">'''</span><span class="p">)</span>

</pre></td></tr></tbody></table></code></pre></div></div>
<p>Output:</p>
<pre><code class="language-plain">Epoch: 0
Accuracy: 61.85476815398076
Loss: 0.6853206089892516
Epoch: 1
Accuracy: 75.32808398950131
Loss: 0.6586288675235639
Epoch: 2
Accuracy: 82.76465441819772
Loss: 0.6005860694854382
### TESTING ###
        Accuracy: 82.87
        Loss: 0.5592
...
...
...
Epoch: 27
Accuracy: 95.18810148731409
Loss: 0.3622470853209808
Epoch: 28
Accuracy: 95.2755905511811
Loss: 0.3614634818813828
Epoch: 29
Accuracy: 95.53805774278216
Loss: 0.3607256871925981
### TESTING ###
        Accuracy: 92.31
        Loss: 0.394
</code></pre>

<p>As you can see, with just 30 epochs of training we are able to achieve testing accuracy of more than <b>92%</b>! The RNN implementation we saw above is just to provide insight into the working of RNNs and I don’t recommend anyone to actually build their own RNNs while working on their projects. We will now use the <code class="language-plaintext highlighter-rouge">torch.nn.RNN</code> module on the same problem. This is a much faster implementation which supports parallel processing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">NameClassifier</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">65</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="n">dropout_prob</span> <span class="o">=</span> <span class="mf">0.4</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sequence_length</span> <span class="o">=</span> <span class="n">max_len</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">RNN</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">input_size</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> 
            <span class="n">num_layers</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span> 
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout_prob</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">sequence_length</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="n">rnn_output</span><span class="p">,</span> <span class="n">new_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="n">rnn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout_layer</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">)</span>
        <span class="n">linear_output</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">sequence_length</span><span class="p">)))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">linear_output</span><span class="p">))</span>
        <span class="n">new_hidden</span> <span class="o">=</span> <span class="n">new_hidden</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">new_hidden</span>
        
        
    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>The only difference between this model and the one which we built is the addition of <code class="language-plaintext highlighter-rouge">nn.Dropout</code> which helps the model to generalize better and prevents overfitting. For this particular problem this addition did not add much of a difference to the results but it is good practice to have some sort of regularization and it does not harm our results in any way. Setting hyperparameters and training:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="n">model</span> <span class="o">=</span> <span class="n">NameClassifier</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">5e-6</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">max_accuracy</span> <span class="o">=</span> <span class="mf">0.0</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>This is what the <code class="language-plaintext highlighter-rouge">nn.RNN</code> model looks like.</p>

<p>Output:</p>
<pre><code class="language-plain">NameClassifier(
    (rnn): RNN(65, 256, num_layers=2, batch_first=True, dropout=0.4)
    (dropout_layer): Dropout(p=0.4)
    (linear_layer): Linear(in_features=4608, out_features=256, bias=True)
    (output_layer): Linear(in_features=256, out_features=2, bias=True)
)
</code></pre>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Training the model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
</pre></td><td class="rouge-code"><pre><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span> <span class="k">continue</span>
        <span class="n">model</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_batch</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">hidden</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>    
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nb">long</span><span class="p">().</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)).</span><span class="n">item</span><span class="p">()</span>
    
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Accuracy: </span><span class="si">{</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span> <span class="o">*</span> <span class="mi">100</span><span class="si">}</span><span class="se">\n</span><span class="s">Loss: </span><span class="si">{</span><span class="n">epoch_loss</span><span class="o">/</span><span class="n">total</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">test_epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span> <span class="k">continue</span>
                <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_batch</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">hidden</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nb">long</span><span class="p">().</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>
                <span class="n">test_epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)).</span><span class="n">item</span><span class="p">()</span>

        <span class="n">test_accuracy</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'''### TESTING ###
        Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">}</span><span class="s">
        Loss: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">test_epoch_loss</span><span class="o">/</span><span class="n">total</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s">'''</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">max_accuracy</span> <span class="o">&lt;</span> <span class="n">test_accuracy</span><span class="p">:</span>
            <span class="n">max_accuracy</span> <span class="o">=</span> <span class="n">test_accuracy</span>
            <span class="n">save_weights</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">MODEL_PATH</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Best model found!'</span><span class="p">)</span>
        
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Output:</p>
<pre><code class="language-plain">Epoch: 0
Accuracy: 51.58450704225353
Loss: 0.086549880848804
Epoch: 1
Accuracy: 51.6725352112676
Loss: 0.08647492449258415
Epoch: 2
Accuracy: 51.76056338028169
Loss: 0.08638013862598111
### TESTING ###
        Accuracy: 48.209999999999994
        Loss: 0.0864
Best model found!
...
...
...
Epoch: 27
Accuracy: 91.90140845070422
Loss: 0.049783599733466834
Epoch: 28
Accuracy: 92.6056338028169
Loss: 0.04917106074346623
Epoch: 29
Accuracy: 92.42957746478874
Loss: 0.04905853562161956
### TESTING ###
        Accuracy: 93.57
        Loss: 0.0475
Best model found!
</code></pre>

<p>As you can see we get very similar accuracies from the two models. This is because they are essentially doing the same thing.</p>

<h3 id="testing-the-model-with-user-input">Testing the model with user input</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">names</span><span class="p">.</span><span class="n">one_hot_encoder</span><span class="p">(</span><span class="s">'Tribbiani'</span><span class="p">)</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">input_tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">hidden</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">class_</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'German'</span> <span class="k">if</span> <span class="n">class_</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="s">'Italian'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Confidence: </span><span class="si">{</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">class_</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Output:</p>

<pre><code class="language-plain">Italian
Confidence: 0.9999949932098389
</code></pre>

<hr />

<h2 id="drawbacks-of-the-vanilla-rnn">Drawbacks of the vanilla RNN</h2>

<p>The RNN architecture which we have used until now (commonly called a “vanilla” RNN) is very simple and generally works for shorter sequence lengths. This is in part because of the <b>vanishing gradient</b> problem which affects vanilla RNNs. Due to this problem, the gradient signal from nearby hidden states (wrt time) is much larger when compared with that of farther hidden states. This leads to the model learning closer dependencies but failing to capture long term dependencies. <br />
For example:<br />
Q. The author of these books <strong>__</strong>_____ coming to town.<br /></p>
<ul>
  <li>is</li>
  <li>are</li>
</ul>

<p>The answer is of course ‘is’. This is because we are referring to the author and not to the books. This is called <strong>syntactic recency</strong>. The problem with RNNs is that they are able to identify sequential recency and might output ‘are’ as the blank follows the word ‘books’. Another reason as to why RNNs are unable to capture long term dependencies is that the hidden state is constantly being rewritten. This leads to continuous loss of information. Hence there is a need for better architectures which would be able to model short term as well as long term dependencies. There are two popularly used architectures namely:</p>
<ol>
  <li>Gated Recurrent Units (GRUs)</li>
  <li>Long Short Term Memory (LSTMs)</li>
</ol>

<h3 id="grus">GRUs</h3>

<p><img src="/assets/images/posts/RNNs/GRU.png" /></p>

<p>Here at each timestep \(t\), we have the input \(x^t\) and the hidden state \(h^t\). The GRU makes use of 2 gates:</p>
<ol>
  <li>
    <p>The update gate:<br />
\(\large u^{(t)} = \sigma(W_u * h^{(t-1)} + U_u * x^{(t)} + b_u)\)<br />
Controls which parts of the hidden state are updated and which are preserved.</p>
  </li>
  <li>
    <p>The reset gate:<br />
\(\large r^{(t)} = \sigma(W_r * h^{(t-1)} + U_r * x^{(t)} + b_r)\)<br />
Controls which parts of the previous hidden state are used to calculate new content.</p>
  </li>
</ol>

<p>These gates can be thought of as small Neural Networks which are used to calculate and extract relevant features and information from the input.</p>

<p>The reset gate is directly used to calculate the new hidden state content:<br />
\(\large \tilde{h} = tanh\Big(W_h * (r^{(t)} \bullet h^{(t-1)}) + U_h * x^{(t)} +b_h\Big)\)</p>

<p>The new hidden state is calculated using the update gate. It simulatneously keeps what is kept from the previous hidden state and what is updated to the new hidden state.</p>

\[\large h^{(t)} = (1 - u^{(t)}) \bullet h^{(t-1)} + u^{(t)} \bullet \tilde{h}^{(t)}\]

<h4 id="how-does-a-gru-solve-the-vanishing-gradient-problem">How does a GRU solve the vanishing gradient problem?</h4>

<p>GRUs make it easier to retain information long term. This can be done through the update gate. If the update gate is set to \(0\), the value of the new hidden state will become:<br />
\(\large h^{(t)} = (1 - u^{(t)}) \bullet h^{(t-1)} + u^{(t)} \bullet \tilde{h}^{(t)}\)<br />
But \(u^{(t)} = 0\)<br />
Hence, <br />
\(\large h^{(t)} = h^{(t-1)}\)</p>

<p>This means that the hidden state will never change. Hence from this example we can understand how the GRU will be able to capture long term or short term dependencies as it suites the problem.</p>

<h3 id="lstms">LSTMs</h3>

<p><img src="/assets/images/posts/RNNs/LSTM.png" /></p>

<p>LSTMs are older and slightly more complex as compared to GRUs. They attempt to solve the vanishing gradient problem by having a separate memory called the <b>cell state</b>. This is separate from the hidden state. Theoretically this cell state can save information about the entire sequence. LSTMs use different gates to define how the cell state and hidden state are updated. Performance wise, there is no clear better alternative to use between GRUs and LSTMs. They have both outperformed each other at different tasks. GRUs due to their simpler structure are slightly faster to train and also easier to understand. Let us understand the working of the LSTM gates.</p>

<ol>
  <li>
    <p>Forget gate<br />
In this gate the input and \(h_{t-1}\) is passed through a sigmoid function. This squishes the inputs between \(0\) and \(1\). The gate will “forget” values closer to \(0\) and “remember” values closer to \(1\).</p>
  </li>
  <li>
    <p>Input gate<br />
This gate works in a similar fashion to the forget gate. It is used to extract relevant features from the input data. The output of this gate (from a sigmoid function) is again used to decide which parts of the input are important.</p>
  </li>
  <li>
    <p>Output gate<br />
The output gate takes the cell state and the previous hidden state as input and calculates what the next hidden state should be.</p>
  </li>
  <li>
    <p>Cell state<br />
The new cell state \(C_t\) is calculated using the outputs of the forget gate and the input gate.</p>
  </li>
</ol>

<h3 id="code">Code</h3>

<p>Using both LSTMs and GRUs in pytorch is very easy using <code class="language-plaintext highlighter-rouge">nn.LSTM</code> and <code class="language-plaintext highlighter-rouge">nn.GRU</code>. They follow a very similar API to that of <code class="language-plaintext highlighter-rouge">nn.RNN</code> and can provide better results for complex problems without much modification to the rest of the code.</p>

<h3 id="additional-material">Additional material</h3>

<ol>
  <li>I have written another blog to build on your understanding using a project! Check out <a href="/Character-Level-Language-Model/">character based language model</a> using RNNs.</li>
  <li>Chris Olah’s <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">post</a> on RNNs and LSTMs.</li>
  <li>Andrej Karpathy’s <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">post</a> on building an RNN from scratch.</li>
  <li>A great <a href="https://www.youtube.com/watch?v=l1rlFh0PmZw&amp;list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&amp;index=9">lecture</a> by Rachel Thomas as part of “A Code-First Introduction to Natural Language Processing”.</li>
</ol>


            </div>

            <!-- Rating -->
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2020-02-18">18 Feb 2020</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="/categories#PyTorch">PyTorch</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#Python">Python</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#RNNs">RNNs</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="//Character-Level-Language-Model/"> &laquo; Character level language model using LSTMs</a>
            
            
            <div class="clearfix"></div>
            </div> -->
            <!-- End Categories -->

        </div>
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'anikets-portfolio'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

</div>


<!-- Bottom Alert Bar
================================================== -->
<!-- <div class="alertbar">
	<div class="container text-center">
		<span><img src="/assets/images/logo.png" alt="Aniket Sanap"> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="https://wowthemes.us11.list-manage.com/subscribe/post?u=8aeb20a530e124561927d3bd8&amp;id=8c3d2d214b" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
	</div>
</div> -->

    
</div>

<!-- Categories Jumbotron
================================================== -->
<!-- <div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="/categories#Vision">Vision (2)</a>
                
                    <a class="mt-1 mb-1" href="/categories#CNN">CNN (2)</a>
                
                    <a class="mt-1 mb-1" href="/categories#PyTorch">PyTorch (4)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Python">Python (4)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Faiss">Faiss (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#NLP">NLP (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#LSTM">LSTM (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#RNNs">RNNs (1)</a>
                
            
            
		</div>
	</div>
</div> -->


<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-12 col-sm-12 text-center text-lg-center">
                <!-- Copyright © 2021 Aniket Sanap  -->
                <!-- Made with jekyll -->
            </div>
            <!-- <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div> -->
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="/assets/js/mediumish.js"></script>



<script src="/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//anikets-portfolio.disqus.com/count.js"></script>


</body>
</html>
