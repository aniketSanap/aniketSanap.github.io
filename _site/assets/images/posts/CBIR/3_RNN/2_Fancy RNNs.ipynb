{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks of the vanilla RNN\n",
    "\n",
    "The RNN architecture which we have used until now (commonly called a \"vanilla\" RNN) is very simple and fails when the sequence length is increased. This is in part because of the <b>vanishing gradient</b> problem which affects vanilla RNNs. Due to this problem, the gradient signal from nearby hidden states (wrt time) is much larger when compared with that of farther hidden states. This leads to the model learning closer dependencies but failing to capture long term dependencies. <br>\n",
    "For example:<br>\n",
    "Q. The chef who cooked these dishes ___________ learnt to cook from his mother.<br>\n",
    "- is\n",
    "- are\n",
    "\n",
    "The answer is of course 'is'. This is because we are referring to the chef and not to the dishes. This is called syntactic recency. The problem with RNNs is that they are able to identify sequential recency and might output 'are' as the blank follows the word 'dishes'. Another reason as to why RNNs are unable to capture long term dependencies is that the hidden state is constantly being rewritten. This leads to continuous loss of information. Hence there was a need for better architectures which would be able to model short term as well as long term dependencies. There are two popularly used architectures namely\n",
    "1. Gated Recurrent Units (GRUs)\n",
    "2. Long Short Term Memory (LSTMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRUs\n",
    "\n",
    "<img src='Images/GRU.png'>\n",
    "\n",
    "Here at each timestep $t$, we have the input $x^t$ and the hidden state $h^t$. The GRU makes use of 2 gates:\n",
    "1. The update gate:<br>\n",
    "$\\large u^{(t)} = \\sigma(W_u * h^{(t-1)} + U_u * x^{(t)} + b_u)$<br>\n",
    "Controls which parts of the hidden state are updated and which are preserved.\n",
    "\n",
    "2. The reset gate:<br>\n",
    "$\\large r^{(t)} = \\sigma(W_r * h^{(t-1)} + U_r * x^{(t)} + b_r)$<br>\n",
    "Controls which parts of the previous hidden state are used to calculate new content.\n",
    "\n",
    "These gates can be thought of as small Neural Networks which are used to calculate and extract relevant features and information from the input.\n",
    "\n",
    "The reset gate is directly used to calculate the new hidden state content:<br>\n",
    "$\\large \\tilde{h} = tanh\\Big(W_h * (r^{(t)} \\bullet h^{(t-1)}) + U_h * x^{(t)} +b_h\\Big)$\n",
    "\n",
    "The new hidden state is calculated using the update gate. It simulatneously keeps what is kept from the previous hidden state and what is updated to the new hidden state.\n",
    "\n",
    "$\\large h^{(t)} = (1 - u^{(t)}) \\bullet h^{(t-1)} + u^{(t)} \\bullet \\tilde{h}^{(t)}$\n",
    "\n",
    "#### How does a GRU solve the vanishing gradient problem?\n",
    "\n",
    "GRUs make it easier to retain information long term. This can be done through the update gate. If the update gate is set to 0, the value of the new hidden state will become:<br>\n",
    "$\\large h^{(t)} = (1 - u^{(t)}) \\bullet h^{(t-1)} + u^{(t)} \\bullet \\tilde{h}^{(t)}$<br>\n",
    "But $u^{(t)} = 0$<br>\n",
    "Hence, <br>\n",
    "$\\large h^{(t)} = h^{(t-1)}$\n",
    "\n",
    "This means that the hidden state will never change. Hence from this example we can understand how the GRU will be able to capture long term or short term dependencies as it suites the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs\n",
    "\n",
    "<img src='Images/LSTM.png' />\n",
    "\n",
    "LSTMs are older and slightly more complex as compared to GRUs. They attempt to solve the vanishing gradient problem by having a separate memory called the <b>cell state</b>. This is separate from the hidden state. Theoretically this cell state can save information about the entire sequence. LSTMs use different gates to define how the cell state and hidden state are updated. Performance wise, there is no clear better alternative to use between GRUs and LSTMs. They have both outperformed each other at different tasks. GRUs due to their simpler structure are slightly faster to train and also easier to understand. Let us understand the working of the LSTM gates.\n",
    "\n",
    "1. Forget gate\n",
    "In this gate the input and $h_{t-1}$ is passed through a sigmoid function. This squishes the inputs between 0 and 1. The gate will \"forget\" values closer to 0 and \"remember\" values closer to 1.\n",
    "\n",
    "2. Input gate\n",
    "This gate works in a similar fashion to the forget gate. It is used to extract relevant features from the input data. The output of this gate (from a sigmoid function) is again used to decide which parts of the input are important.\n",
    "\n",
    "3. Output gate\n",
    "The output gate takes the cell state and the previous hidden state as input and calculates what the next hidden state should be. \n",
    "\n",
    "4. Cell state\n",
    "The new cell state $C_t$ is calculated using the outputs of the forget gate and the input gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "Using both LSTMs and GRUs in pytorch is very easy using `nn.LSTM` and `nn.GRU`. They follow a very similar API to that of `nn.RNN` and can provide better results for complex problems without much modification to the rest of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects\n",
    "\n",
    "1. [Character based language model](nbs/CharacterLevelLanguageModel.ipynb)\n",
    "2. [Word based language model](nbs/WordLevelLanguageModel.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
