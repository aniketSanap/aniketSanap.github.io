
var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about",
    "title": "About me",
    "body": "Welcome to my site! I'm a machine learning enthusiast with an affinity for research. I enjoy programming and learning new technologies. I worked as a machine learning engineer at AI Adventures till March 2020. Before that I completed my Bachelor's of Engineering in Computer Engineering from Maharashtra Institute of Technology, Pune. I am currently looking for jobs and internship opportunities in this domain. Get in touch!You can contact me on linkedin "
    }, {
    "id": 2,
    "url": "http://localhost:4000/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "http://localhost:4000/",
    "title": "Home",
    "body": " Machine learning enthusiast, blog writer, novice guitarist.       Featured Blogs:                                                                                                                                                                                                           Understanding RNNs                              :               Understand the working of this powerful deep learning architecture:                                                                       18 Feb 2020                &lt;/span&gt;                                                                                                                                                                                                                                                                                                  Character level language model using LSTMs                              :               Predicting the next character in a sequence:                                                                       28 Dec 2019                &lt;/span&gt;                                                                                                                                                                                                                                                                                                        Content Based Image Retrieval (CBIR)                              :               Retrieving similar images based on the content of a query image:                                                                       28 Dec 2019                &lt;/span&gt;                                                                                                      Additional Projects:                                                                                                     Neural Style Transfer              :       Transferring the style of one image to another using neural networks:                               28 Dec 2019        &lt;/span&gt;                                    "
    }, {
    "id": 4,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 5,
    "url": "http://localhost:4000/Understanding-RNNs/",
    "title": "Understanding RNNs",
    "body": "2020/02/18 - Recurrent Neural Networks: Linear layers and convolutional layers are very popular in neural network architectures. So what is the need of Recurrent Neural Networks? There is an obvious limitation to the former architectures. Can you guess what it is?. . . . Memory!These architectures don’t store any information about the previous inputs given to the network. This mean they tend to give poor results while working with sequential data (for the most part). Humans don’t start thinking from scratch at every instant. Just while reading this sentence, you have an idea of the words which came before and the ones to follow. A linear model processes each input independently. So you must convert the entire sequence into one input data point. Hence, they are stateless. What is an RNN?: An RNN is an architecture which unlike Linear models, preserve state. They process sequences by iterating through its elements and maintaining a state. This state is reset while processing two different sequences. This is what a simple RNN looks like: The saved state is called the hidden state. An RNN processes each element of the sequence sequentially. At each time step, it updates its hidden state and produces an output. This is what happens when we unroll an RNN: Unrolling an RNN is simply visualizing how it processes the sequence element by element. In reality, the RNN consists of just one cell processing the input in a loop. This property of an RNN allows it to process variable length inputs. RNNs are just a refactored, fully-connected neural network. The working of an RNN (at timestep ) is as follows:An RNN consists of 3 weight matrices: , , .   is the weight matrix for the input (x).  is the weight matrix for the hidden state.  is the weight matrix for the output. The hidden state is given by:  is the hidden state at timestep .  is the activation function (generally sigmoid or tanh).  is the input at the current timestep. The output of the RNN is given by: Due to the sequential nature of natural language, RNNs are commonly used in Natural Language Processing. Let us try to better understand the working of RNNs using an example. In this example we are going to build a model to classify names into two countries of origin -&gt; Italy and Germany. Our dataset consists of two files Italian. txt and German. txt. Both of these files contain a single name on each line. The data can be downloaded by: 1wget https://download. pytorch. org/tutorial/data. zipAfter unzipping the downloaded file, you will find several files which follow the following format: name_1name_2name_3. . . Cleaning the Data: We start off by importing all the modules we will be using for this project. The only module you will need to install is of course pytorch. 1234567import torchfrom torch import nnfrom torch. nn import functional as Ffrom torch. utils. data import Dataset, DataLoader, random_splitfrom pprint import pprintimport osfrom string import ascii_lettersReading the data: 1234567with open('Projects/NameClassifier/data/names/German. txt', 'r') as german_f, open('Projects/NameClassifier/data/names/Italian. txt', 'r') as italian_f:  german_names = german_f. read()  italian_names = italian_f. read()print(f'German names:\n{german_names[:30]}')print()print(f'Italian names:\n{italian_names[:33]}')Output: German names:AbbingAbelAbelnAbtAchillesItalian names:AbandonatoAbatangeloAbatantuonoFinding all the unique characters in the files: The classifier which we are going to build is going to be character based. This means that it will take a sequence of characters as its input. Each name will be read by the model character by character. For this we need to first find all the unique characters in the files. We find all the unique characters in the files and then take its union with all letters (uppercase and lowercase) to form our vocabulary. 123unique_characters = set((german_names + italian_names). replace('\n', '')). union(set(ascii_letters))unique_characters = list(unique_characters)''. join(sorted(unique_characters))Output:   'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzßàäèéìòóöùü Doing this ensures that our vocabulary will contain as many used characters as possible. You can probably get away with just using ascii_letters to form your vocabulary. This is what our list of names looks like: 1234german_names = german_names. split('\n')italian_names = italian_names. split('\n')print(german_names[:5])print(italian_names[:5])Output: ['Abbing', 'Abel', 'Abeln', 'Abt', 'Achilles']['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']Removing common names: We don’t want our classifier to accept the same input with two different classes. Hence, we will find the names which exist in both the german and italian datasets and remove them from both 12common_names = list(set(german_names). intersection(set(italian_names)))common_namesOutput: ['', 'Salomon', 'Paternoster']After removing the common names: 123456for common_name in common_names:  german_names. remove(common_name)  italian_names. remove(common_name)  common_names = list(set(german_names). intersection(set(italian_names)))common_namesOutput: []Creating our data: We will create a list of all our names. This will be the input passed to our model. Along with this we will also need labels. We will have a label of 0 for german names and a label of 1 for italian names. 1234567german_label = [0]italian_label = [1]all_names = german_names + italian_namesall_labels = german_label * len(german_names) + italian_label * len(italian_names)print(all_names[720:726])print(all_labels[720:726])Output: ['Zimmerman', 'Zimmermann', 'Abandonato', 'Abatangelo', 'Abatantuono', 'Abate'][0, 0, 1, 1, 1, 1]One hot encoding characters: For our model to be able to process our input, we have to convert the characters to one hot encoded vectors. The size of our vector will be the total number of unique characters in our dataset. Hence we will first create a mapping of our character and its index. We can then use this mapping to convert our input characters to digits. 12stoi = {char:idx for idx, char in enumerate(sorted(unique_characters))}stoiOutput: {' ': 0, ' : 1,'A': 2,'B': 3,'C': 4,'D': 5,'E': 6,'F': 7,'G': 8,'H': 9,'I': 10,'J': 11,'K': 12,'L': 13,'M': 14,'N': 15,'O': 16,'P': 17,'Q': 18,'R': 19,'S': 20,'T': 21,'U': 22,'V': 23,'W': 24,'X': 25,'Y': 26,'Z': 27,'a': 28,'b': 29,'c': 30,'d': 31,'e': 32,'f': 33,'g': 34,'h': 35,'i': 36,'j': 37,'k': 38,'l': 39,'m': 40,'n': 41,'o': 42,'p': 43,'q': 44,'r': 45,'s': 46,'t': 47,'u': 48,'v': 49,'w': 50,'x': 51,'y': 52,'z': 53,'ß': 54,'à': 55,'ä': 56,'è': 57,'é': 58,'ì': 59,'ò': 60,'ó': 61,'ö': 62,'ù': 63,'ü': 64}While our RNN can accept inputs of variable length, we still have to define a sequence length. This will allow us to batch our data for parallel execution. 123456789101112131415161718192021def one_hot_encoder(name, sequence_length):  global stoi  size = len(stoi)  print(f'Size of stoi: {size}')  # To save output  encoded = []  # Iterating through name  for char in name:    temp = torch. zeros(size)    # Setting index of character to 1    temp[stoi[char]] = 1    encoded. append(temp)      # Filling the rest of the sequence with zeros  for i in range(sequence_length - len(name)):    temp = torch. zeros(size)    encoded. append(temp)  return torch. stack(encoded)one_hot_encoder('Aniket', 10)Size of stoi: 65tensor([[0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,      0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,      0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,      0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],    [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,      0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,      0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,      0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],    . . . Creating our dataset object: Now we have done a lot of preprocessing! Let us combine all of this in our dataset class. We will set the sequence length to as that is the length of the longest name in our dataset. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class NameDataset(Dataset):  def __init__(self, german_fname='Projects/NameClassifier/data/names/German. txt', italian_fname='Projects/NameClassifier/data/names/Italian. txt'):    super(). __init__()    # Reading from files    with open(german_fname, 'r') as german_f, open(italian_fname, 'r') as italian_f:      german_names = german_f. read()      italian_names = italian_f. read()        # Finding unique characters    unique_characters = list(set((german_names + italian_names). replace('\n', '')). union(set(ascii_letters)))    german_names = german_names. split('\n')    italian_names = italian_names. split('\n')        # Removing common names    common_names = list(set(german_names). intersection(set(italian_names)))    for common_name in common_names:      german_names. remove(common_name)      italian_names. remove(common_name)    german_label = [0]    italian_label = [1]    # Setting names and labels    self. names = german_names + italian_names    self. labels = german_label * len(german_names) + italian_label * len(italian_names)        # Mapping from chars to int    self. stoi = {char:idx for idx, char in enumerate(sorted(unique_characters))}        # Size of longest word is 18    self. sequence_length = 18        # One hot encoded names    self. encoded_names = self. encode_dataset()  def one_hot_encoder(self, name):    size = len(self. stoi)    encoded = []    for char in name:      temp = torch. zeros(size)      temp[self. stoi[char]] = 1      encoded. append(temp)    for i in range(self. sequence_length - len(name)):      temp = torch. zeros(size)      encoded. append(temp)    return torch. stack(encoded)      def encode_dataset(self):    encoded_list = []    for name in self. names:      encoded_list. append(self. one_hot_encoder(name))          return torch. stack(encoded_list)      def __len__(self):    return len(self. labels)    def __getitem__(self, idx):    return self. encoded_names[idx], torch. tensor([self. labels[idx]])Let us see what our output looks like: 12names = NameDataset()names[0]Output: (tensor([[0. , 0. , 1. , . . . , 0. , 0. , 0. ],      [0. , 0. , 0. , . . . , 0. , 0. , 0. ],      [0. , 0. , 0. , . . . , 0. , 0. , 0. ],      . . . ,      [0. , 0. , 0. , . . . , 0. , 0. , 0. ],      [0. , 0. , 0. , . . . , 0. , 0. , 0. ],      [0. , 0. , 0. , . . . , 0. , 0. , 0. ]]), tensor([0]))12# Shape of input tensor (one word)names[0][0]. shapeOutput: torch. Size([18, 65])12345678device = 'cuda' if torch. cuda. is_available() else 'cpu'split_ratio = 0. 8data_len = len(names)train_size = int(split_ratio * data_len)test_size = data_len - train_size# Randomly splits data into given sizestrain_dataset, test_dataset = random_split(names, lengths=(train_size, test_size))Comparison with a Linear model: Before we build our RNN based model, let us look at the results with a conventional Linear model. On our problem statement, using a model with just 3 linear layers, I was able to achieve an accuracy of just 69. 2% even after training for 100 epochs. This problem is very simple with very short sequences. For a problem with longer sequences, the model’s performance would be even worse. Building an RNN using linear layers: Let us revisit the mathematics behind an RNN: Where is the hidden state at timestep and is the output of the RNN. For illustration purposes we will assume the batch size to be . Let us discuss some of the terminology commonly used while talking about RNNs. Sequence lengthAlthough RNNs can handle inputs of varying sequence lengths, tensor operations require the every sequence of the same size. This is why while RNNs can accept sequences of any length, for parallel processing we will maintain the same sequence length for every input batch. The sequence length for our model is because that is the length of the longest name in our dataset. Input sizeThis parameter of our RNN is different from the sequence length. This parameter signifies the size of one element of the sequence. For our example, the input size is which is the size of our one hot encoded vector. Now let us try building our own RNN with just linear layers. We will use nn. Linear for a linear layer. We need a linear layer for and each. We will also initialize the hidden state with torch. zeros. We will have a for loop to iterate through the sequence. 1234567891011121314151617181920212223242526272829303132class LinearRNN(nn. Module):  def __init__(self):    super(). __init__()    global device    self. device = device    self. hidden_size = 256    self. sequence_length = 18    self. input_size = 65    self. Wx = nn. Linear(self. input_size, self. hidden_size)    self. Wh = nn. Linear(self. hidden_size, self. hidden_size)    self. Wy = nn. Linear(self. sequence_length * self. hidden_size, self. hidden_size)    self. h = torch. zeros(1, self. hidden_size). to(self. device)    self. output_layer = nn. Linear(self. hidden_size, 2)      def forward(self, input_tensor):    h = torch. zeros(1, self. hidden_size). to(self. device)    res = []    # input_tensor. shape[1] = sequence length    for i in range(input_tensor. shape[1]):         # input_tensor[:, i] = the ith element in the sequence      x = F. tanh(self. Wx(input_tensor[:, i]))         h = F. tanh(self. Wh(h))      h = torch. add(h, x)      res. append(h)        self. h = h. detach()        res = torch. stack(res, dim=1)    res = res. reshape(-1, self. sequence_length * self. hidden_size)    res = F. relu(self. Wy(res))    res = self. output_layer(res)    return res  Now let us create our DataLoader and set some hyperparameters before training the model. 123456789101112131415batch_size = 1linear_train_loader = DataLoader(train_dataset, batch_size=batch_size)linear_test_loader = DataLoader(test_dataset, batch_size=batch_size)model = LinearRNN(). to(device)criterion = nn. CrossEntropyLoss()lr = 5e-6optimizer = torch. optim. Adam(model. parameters(), lr=lr)num_epochs = 30max_accuracy = 0. 0MODEL_PATH = ''if os. path. exists(MODEL_PATH):  print('Existing model found!')  load_weights(model, MODEL_PATH)else:  print('No existing model found. ')This is what our model looks like: No existing model found. LinearRNN(  (Wx): Linear(in_features=65, out_features=256, bias=True)  (Wh): Linear(in_features=256, out_features=256, bias=True)  (Wy): Linear(in_features=4608, out_features=256, bias=True)  (output_layer): Linear(in_features=256, out_features=2, bias=True))Now it time to finally train the model. 12345678910111213141516171819202122232425262728293031323334353637for epoch in range(num_epochs):  model. train()  epoch_loss = 0  total = 0  correct = 0  print(f'Epoch: {epoch}')  for input_batch, labels in linear_train_loader:    if labels. size(0) != batch_size: continue    model. zero_grad()    output = model. forward(input_batch. to(device))    loss = criterion(output, labels. to(device). long(). reshape(batch_size,))    loss. backward()    optimizer. step()    epoch_loss += loss. item()    total += labels. size(0)    correct += torch. sum(torch. argmax(output, dim=1). view(1, -1) == labels. to(device). view(1, -1)). item()  print(f'Accuracy: {correct/total * 100}\nLoss: {epoch_loss/total}')  if (epoch + 1) % 3 == 0:    test_epoch_loss = 0    total = 0    correct = 0    model. eval()    for input_batch, labels in linear_test_loader:      with torch. no_grad():        if labels. size(0) != batch_size: continue        output = model. forward(input_batch. to(device))        loss = criterion(output, labels. to(device). long(). reshape(batch_size,))        test_epoch_loss += loss. item()        total += labels. size(0)        correct += torch. sum(torch. argmax(output, dim=1). view(1, -1) == labels. to(device). view(1, -1)). item()    test_accuracy = round(correct/total, 4) * 100    print(f'''### TESTING ###    Accuracy: {test_accuracy}    Loss: {round(test_epoch_loss/total, 4)}''')Output: Epoch: 0Accuracy: 61. 85476815398076Loss: 0. 6853206089892516Epoch: 1Accuracy: 75. 32808398950131Loss: 0. 6586288675235639Epoch: 2Accuracy: 82. 76465441819772Loss: 0. 6005860694854382### TESTING ###    Accuracy: 82. 87    Loss: 0. 5592. . . . . . . . . Epoch: 27Accuracy: 95. 18810148731409Loss: 0. 3622470853209808Epoch: 28Accuracy: 95. 2755905511811Loss: 0. 3614634818813828Epoch: 29Accuracy: 95. 53805774278216Loss: 0. 3607256871925981### TESTING ###    Accuracy: 92. 31    Loss: 0. 394As you can see, with just 30 epochs of training we are able to achieve testing accuracy of more than 92%! The RNN implementation we saw above is just to provide insight into the working of RNNs and I don’t recommend anyone to actually build their own RNNs while working on their projects. We will now use the torch. nn. RNN module on the same problem. This is a much faster implementation which supports parallel processing. 12345678910111213141516171819202122232425262728293031class NameClassifier(nn. Module):  def __init__(self, max_len=18, hidden_size=256, input_size=65):    super(). __init__()    dropout_prob = 0. 4    self. input_size = input_size    self. sequence_length = max_len    self. hidden_size = hidden_size    self. num_layers = 2    self. rnn = nn. RNN(      input_size=self. input_size,      hidden_size=self. hidden_size,       num_layers=self. num_layers,       batch_first=True,      dropout=dropout_prob    )    self. dropout_layer = nn. Dropout(dropout_prob)    self. linear_layer = nn. Linear(self. hidden_size * self. sequence_length, 256)    self. output_layer = nn. Linear(256, 2)          def forward(self, input_tensor, hidden):    rnn_output, new_hidden = self. rnn(input_tensor, hidden)    rnn_output = self. dropout_layer(rnn_output)    linear_output = F. relu(self. linear_layer(rnn_output. reshape(-1, self. hidden_size * self. sequence_length)))    output = F. softmax(self. output_layer(linear_output))    new_hidden = new_hidden. detach()    return output, new_hidden          def init_hidden(self, batch_size):    return torch. zeros(self. num_layers, batch_size, self. hidden_size)The only difference between this model and the one which we built is the addition of nn. Dropout which helps the model to generalize better and prevents overfitting. For this particular problem this addition did not add much of a difference to the results but it is good practice to have some sort of regularization and it does not harm our results in any way. Setting hyperparameters and training: 123456model = NameClassifier(). to(device)criterion = nn. CrossEntropyLoss()lr = 5e-6optimizer = torch. optim. Adam(model. parameters(), lr=lr)num_epochs = 30max_accuracy = 0. 0This is what the nn. RNN model looks like. Output: NameClassifier(  (rnn): RNN(65, 256, num_layers=2, batch_first=True, dropout=0. 4)  (dropout_layer): Dropout(p=0. 4)  (linear_layer): Linear(in_features=4608, out_features=256, bias=True)  (output_layer): Linear(in_features=256, out_features=2, bias=True))123batch_size = 8train_loader = DataLoader(train_dataset, batch_size=batch_size)test_loader = DataLoader(test_dataset, batch_size=batch_size)Training the model: 12345678910111213141516171819202122232425262728293031323334353637383940414243for epoch in range(num_epochs):  model. train()  epoch_loss = 0  total = 0  correct = 0  hidden = model. init_hidden(batch_size=batch_size)  print(f'Epoch: {epoch}')  for input_batch, labels in train_loader:    if labels. size(0) != batch_size: continue    model. zero_grad()    output, hidden = model. forward(input_batch. to(device), hidden. to(device))      loss = criterion(output, labels. to(device). long(). reshape(batch_size,))    loss. backward()    optimizer. step()    epoch_loss += loss. item()    total += labels. size(0)    correct += torch. sum(torch. argmax(output, dim=1). view(1, -1) == labels. to(device). view(1, -1)). item()    print(f'Accuracy: {correct/total * 100}\nLoss: {epoch_loss/total}')  if (epoch + 1) % 3 == 0:    test_epoch_loss = 0    total = 0    correct = 0    model. eval()    hidden = model. init_hidden(batch_size=batch_size)    for input_batch, labels in test_loader:      with torch. no_grad():        if labels. size(0) != batch_size: continue        output, hidden = model. forward(input_batch. to(device), hidden. to(device))        loss = criterion(output, labels. to(device). long(). reshape(batch_size,))        test_epoch_loss += loss. item()        total += labels. size(0)        correct += torch. sum(torch. argmax(output, dim=1). view(1, -1) == labels. to(device). view(1, -1)). item()    test_accuracy = round(correct/total, 4) * 100    print(f'''### TESTING ###    Accuracy: {test_accuracy}    Loss: {round(test_epoch_loss/total, 4)}''')    if max_accuracy &lt; test_accuracy:      max_accuracy = test_accuracy      save_weights(model, MODEL_PATH)      print('Best model found!')    Output: Epoch: 0Accuracy: 51. 58450704225353Loss: 0. 086549880848804Epoch: 1Accuracy: 51. 6725352112676Loss: 0. 08647492449258415Epoch: 2Accuracy: 51. 76056338028169Loss: 0. 08638013862598111### TESTING ###    Accuracy: 48. 209999999999994    Loss: 0. 0864Best model found!. . . . . . . . . Epoch: 27Accuracy: 91. 90140845070422Loss: 0. 049783599733466834Epoch: 28Accuracy: 92. 6056338028169Loss: 0. 04917106074346623Epoch: 29Accuracy: 92. 42957746478874Loss: 0. 04905853562161956### TESTING ###    Accuracy: 93. 57    Loss: 0. 0475Best model found!As you can see we get very similar accuracies from the two models. This is because they are essentially doing the same thing. Testing the model with user input: 123456789with torch. no_grad():  model. eval()  hidden = model. init_hidden(batch_size=1)  input_tensor = names. one_hot_encoder('Tribbiani')  input_tensor = input_tensor. view(1, *input_tensor. shape)  output = model. forward(input_tensor. to(device), hidden. to(device))  class_ = torch. argmax(output[0], dim=1). item()  print('German' if class_ == 0 else 'Italian')  print(f'Confidence: {output[0][0][class_]}')Output: ItalianConfidence: 0. 9999949932098389Drawbacks of the vanilla RNN: The RNN architecture which we have used until now (commonly called a “vanilla” RNN) is very simple and generally works for shorter sequence lengths. This is in part because of the vanishing gradient problem which affects vanilla RNNs. Due to this problem, the gradient signal from nearby hidden states (wrt time) is much larger when compared with that of farther hidden states. This leads to the model learning closer dependencies but failing to capture long term dependencies. For example:Q. The author of these books _______ coming to town.  is areThe answer is of course ‘is’. This is because we are referring to the author and not to the books. This is called syntactic recency. The problem with RNNs is that they are able to identify sequential recency and might output ‘are’ as the blank follows the word ‘books’. Another reason as to why RNNs are unable to capture long term dependencies is that the hidden state is constantly being rewritten. This leads to continuous loss of information. Hence there is a need for better architectures which would be able to model short term as well as long term dependencies. There are two popularly used architectures namely:  Gated Recurrent Units (GRUs) Long Short Term Memory (LSTMs)GRUs: Here at each timestep , we have the input and the hidden state . The GRU makes use of 2 gates:    The update gate:Controls which parts of the hidden state are updated and which are preserved.     The reset gate:Controls which parts of the previous hidden state are used to calculate new content.  These gates can be thought of as small Neural Networks which are used to calculate and extract relevant features and information from the input. The reset gate is directly used to calculate the new hidden state content: The new hidden state is calculated using the update gate. It simulatneously keeps what is kept from the previous hidden state and what is updated to the new hidden state. How does a GRU solve the vanishing gradient problem?: GRUs make it easier to retain information long term. This can be done through the update gate. If the update gate is set to , the value of the new hidden state will become:But Hence, This means that the hidden state will never change. Hence from this example we can understand how the GRU will be able to capture long term or short term dependencies as it suites the problem. LSTMs: LSTMs are older and slightly more complex as compared to GRUs. They attempt to solve the vanishing gradient problem by having a separate memory called the cell state. This is separate from the hidden state. Theoretically this cell state can save information about the entire sequence. LSTMs use different gates to define how the cell state and hidden state are updated. Performance wise, there is no clear better alternative to use between GRUs and LSTMs. They have both outperformed each other at different tasks. GRUs due to their simpler structure are slightly faster to train and also easier to understand. Let us understand the working of the LSTM gates.    Forget gateIn this gate the input and is passed through a sigmoid function. This squishes the inputs between and . The gate will “forget” values closer to and “remember” values closer to .     Input gateThis gate works in a similar fashion to the forget gate. It is used to extract relevant features from the input data. The output of this gate (from a sigmoid function) is again used to decide which parts of the input are important.     Output gateThe output gate takes the cell state and the previous hidden state as input and calculates what the next hidden state should be.     Cell stateThe new cell state is calculated using the outputs of the forget gate and the input gate.  Code: Using both LSTMs and GRUs in pytorch is very easy using nn. LSTM and nn. GRU. They follow a very similar API to that of nn. RNN and can provide better results for complex problems without much modification to the rest of the code. Additional material:  I have written another blog to build on your understanding using a project! Check out character based language model using RNNs.  Chris Olah’s post on RNNs and LSTMs.  Andrej Karpathy’s post on building an RNN from scratch.  A great lecture by Rachel Thomas as part of “A Code-First Introduction to Natural Language Processing”. "
    }, {
    "id": 6,
    "url": "http://localhost:4000/Character-Level-Language-Model/",
    "title": "Character level language model using LSTMs",
    "body": "2019/12/28 - Computer vision had its big boom at the start of this decade with the ImageNet Large Scale Visual Recognition Challenge. Deep CNN architectures were hands down the best approach to tackle vision problems. With the use of transfer learning, these models could be used for a variety of different tasks and applications which led to rapid progress in the field of computer vision. Transfer learning is commonly used in Natural Language Processing through building a language model first and then modifying it to suite our use case. This makes sense because human language is very complex. There is no one correct approach to writing or speaking anything. When we directly train our model on a language specific task, our model has no prior information about the complex structure or any of the nuances of human language. We cannot expect a model which has just been introduced to english to perform well on any language specific task directly without understanding the language first. Hence, we train a language model on a large dataset (like wikipedia) and then use this model which has an understanding of the language on our task. A language model is a probability distribution over a sequence of tokens. In this post we will built a simple character level language model. Hence, the tokens for our model will be characters. Every language model needs a vocabulary. The vocabulary consists of all the tokens which are part of your input and which should be predicted by your output. For a character level language model, the vocabulary consists of every distinct character in your dataset. If you just want the code then you can find it on my github. If you want to understand the working of RNNs and LSTMs, you can check out my other blog post talking about just that. Well then, lets start programming! The corpus: We obviously need a dataset. You can use any large corpus of data to train this model. As this is just a simple character based model, we will be using the text from this site instead of using something like wikipedia. The text looks something like this: 1234567ADVENTURE I.  A SCANDAL IN BOHEMIAI. To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. . . . We will start by reading the file and splitting it into a training set and a small validation set. We will only be using the validation set to keep an eye on it’s loss and making sure that the model does not overfit to the training set. 12345678with open('file. txt', 'r') as f:  text = f. read()text_size = len(text)split_ratio = 0. 9train_text = text[:int(split_ratio * text_size)]test_text = text[int(split_ratio * text_size):]Building the vocabulary: Creating the vocabulary is one of the first steps in preprocessing the text before passing it to your model. For our model, it contains a mapping of all the possible characters with a certain unique integer. The string to int mapping can be a dictionary with the characters as keys and the corresponding indices as values. The integer to character mapping can be a simple list where the index is implied. For a general range of characters you can use from string import printable. printable contains every printable character on your screen and your text probably won’t contain any additional characters. In code it looks like this: 12itos = list(printable)stoi = {char:int_ for int_, char in enumerate(itos)}Preprocessing: One hot encoding: Generally text based tasks need a lot of preprocessing as you can’t pass text directly to your model. In an attempt to keep this model simple, we will use a one hot encoded vector for our input over using word embeddings. The size of our one hot encoded vector will be the size of our vocabulary. For those unaware, a one hot encoded vector is simply a zero vector with a single 1 at the index of our character. We will create a separate one hot vector for every single character as input. Creating the one hot vector of a character will look something like this: 123456789# Finding the index of the input charindex = stoi[char]# Defining the size of the vectorsize = len(itos)# Creating the actual encoded vectorvector = torch. zeros(size)vector[index] = 1Here is our one hot encoder function which will work on an entire string 12345678910111213def one_hot_encoder(sequence, stoi, sequence_length):  size = len(stoi)  encoded = []  for int_ in sequence:    temp = torch. zeros(size)    temp[int_] = 1    encoded. append(temp)  for i in range(sequence_length - len(sequence) - 1):    temp = torch. zeros(size)    encoded. append(temp)  return torch. stack(encoded)You may have noticed that we used sequence_length - len(sequence) - 1 for our loop instead of just sequence_length - len(sequence). This will be explained shortly. Splitting the data into sequences: As our dataset is very large, we will split it into sequences of length sequence_length = 100 each. This will allow us to process multiple sequences in parallel leading to faster training. We will do this splitting as well as the one hot encoding during run time using PyTorch’s datasets and dataloaders. A summary of what we have done so far can be observed through our dataset class. 123456789101112131415161718192021222324252627282930313233343536class LanguageModelDatset(Dataset):  def __init__(self, text, sequence_length=100):    super(). __init__()    self. device = torch. device('cuda' if torch. cuda. is_available() else 'cpu')    self. sequence_length = 100    self. text = text    self. itos = list(printable)    self. stoi = {char:int_ for int_, char in enumerate(self. itos)}    self. text_size = len(self. text)      def one_hot_encoder(self, sequence):    size = len(self. stoi)    encoded = []    for int_ in sequence:      temp = torch. zeros(size)      temp[int_] = 1      encoded. append(temp)    for i in range(self. sequence_length - len(sequence) - 1):      temp = torch. zeros(size)      encoded. append(temp)    return torch. stack(encoded)      def __len__(self):    return self. text_size // self. sequence_length    def __getitem__(self, idx):    sequence = self. text[idx * self. sequence_length:(idx + 1) * self. sequence_length]    sequence = [self. stoi[x] for x in sequence]    x = sequence[:-1]    y = sequence[1:]    x = self. one_hot_encoder(x)        return x, torch. tensor(y)If you don’t know what PyTorch datasets and dataloaders are, I highly recommend you check them out. They are a great way of abstracting your data and PyTorch takes care of a lot of stuff (like parallel loading) under the hood. All you need to know for this post is that you have to define your own __len__() function to return the length of your dataset and your own __getitem__() function which takes an index parameter to return the ith value of your dataset. This allows you to index your dataset just like you would a list. In our __getitem__() function, we have to define our input and label (x and y). Our input is going to be a sequence of characters. The labels for a language model is the same sequence but offset by one word. This is better understood using code. 12input_ = ['H', 'E', 'L', 'L', 'O', ' ', 'W', 'O', 'R', 'L']output = ['E', 'L', 'L', 'O', ' ', 'W', 'O', 'R', 'L', 'D']This is because our language model always tries to predict the next character in the sequence given the previous characters. This is achieved by taking 100 characters from our corpus. The first 99 of which will become our x and the last 99 of which will become our y. 12x = sequence[:-1]y = sequence[1:]This is why we need to have sequence_length - len(sequence) - 1 in our one hot encoder. Alternatively, we can just index 101 characters at a time. Building the model: We will be using an LSTM based architecture for this model. Our input size will be equal to the size of our one hot encoded vector. This is because our model will process one character at a time for each batch. It will do this sequentially for our entire sequence. The size of our output layer will also be equal to the size of our one hot vector. This is because the output tells us which character should be next. So the size has to be equal to the total number of characters in our vocabulary, which is of course, the size of our one hot vector. Here is our model architecture in code: 123456789101112131415161718192021222324252627282930313233class CharLSTM(nn. Module):  def __init__(self, hidden_size=256, n_hidden_layers=3, dropout_prob=0. 5):    super(). __init__()    self. legal_chars = printable    self. hidden_size = hidden_size    self. n_hidden_layers = n_hidden_layers    self. device = torch. device('cuda' if torch. cuda. is_available() else 'cpu')    self. num_layers = n_hidden_layers        self. lstm_layer = nn. LSTM(      input_size=len(self. legal_chars),       hidden_size=hidden_size,       num_layers=n_hidden_layers,       dropout=dropout_prob,       batch_first=True    )    self. dropout_layer = nn. Dropout(dropout_prob)    self. fc = nn. Linear(hidden_size, len(self. legal_chars))    self. to(self. device)  def forward(self, x):    self. hidden = self. hidden. detach()    self. cell = self. cell. detach()    x = x. to(self. device)    lstm_output, (self. hidden, self. cell) = self. lstm_layer(x, (self. hidden, self. cell))    output = self. dropout_layer(lstm_output)    output = output. reshape(-1, self. hidden_size)    return self. fc(output)  def init_hidden(self, batch_size):    # hidden state and cell state    self. hidden, self. cell = [torch. zeros(self. num_layers, batch_size, self. hidden_size). to(self. device),         torch. zeros(self. num_layers, batch_size, self. hidden_size). to(self. device)]A couple of things to note while working with RNNs and their variants:  init_hidden(): This is a method which initializes the hidden and cell state for our LSTM. It will initially be set to zero and will change with every sequence.  As we are keeping the same hidden state throughout, it is necessary to detach it from our graph to prevent BPTT (Backpropagation Through Time) over our entire dataset. It’s time to train!: We will be using the standard pytorch training loop with some modifications.  nn. utils. clip_grad_norm_(model. parameters(), clip): This is used to prevent the exploding gradient problem in RNNs.  Initializing the hiddens state at the start of every epoch.  Ignoring the last batch of every epoch if its size is not equal to our batch_size. This is to prevent issues with hidden state size. 12345678910111213141516171819202122232425262728293031323334353637383940opt = torch. optim. Adam(model. parameters(), lr=lr)criterion = nn. CrossEntropyLoss()print_every = 500for epoch in range(num_epochs):  model. train()  model. init_hidden(batch_size)  counter = 0  for x, y in train_dataloader:    if x. shape[0] != batch_size:      continue    counter += 1    opt. zero_grad()    output = model(x)    loss = criterion(output, y. long(). view(-1,). to(model. device))    loss. backward()    nn. utils. clip_grad_norm_(model. parameters(), clip)    opt. step()    if counter % print_every == 0:      print( Epoch: {}/{}. . .  . format(epoch+1, num_epochs),          Step: {}. . .  . format(counter),          Loss: {:. 4f}. . .  . format(loss. item()))        with torch. no_grad():    model. eval()    test_loss = 0    total = len(test_dataset)    for x, y in test_dataloader:      if x. shape[0] != batch_size:        continue      opt. zero_grad()      output = model(x)      loss = criterion(output, y. long(). view(-1,). to(model. device))      test_loss += loss. item()    print(f'=' * 8 + f'\nTest loss: {test_loss/total}\n' + '=' * 8)    if test_loss &lt; max_test_loss:      max_test_loss = test_loss      save_weights(model, model_path)I trained this model for 30 epochs while making sure that the validation loss was still decreasing. Testing our model: To test our model, we will be writing 2 functions. The first one called predict will take a character and hidden state as input and return the next character and the next hidden state. The second function called generate_chars will take input a sequence of characters using which our model will start its story. This function will call predict n_chars number of times (n_chars will be a parameter which defines the length of the output sequence). 123456789101112131415161718192021222324252627282930313233343536def predict_next_char(model, char, h=None):  global test_dataset  model. eval()  char_vector = test_dataset. one_hot_encoder([char])[0]  # shape: (batch_size, sequence_length, vector_size)  char_vector = char_vector. reshape(1, 1, *char_vector. shape)  with torch. no_grad():    model. hidden, model. cell = h    output = model. forward(char_vector. to(model. device))    h = (model. hidden, model. cell)    probs = F. softmax(output, dim=1)    # Sampling from a distribution to add randomness    dist = torch. distributions. Categorical(probs)    index = dist. sample(). item()    return index, hdef generate_chars(model, n_chars, prime='The'):  global test_dataset  model. eval()  # Batch size: 1  model. init_hidden(1)  h = (model. hidden, model. cell)  for char in [test_dataset. stoi[x] for x in prime]:    _, h = predict_next_char(model, char, h)  chars = []  for i in range(n_chars):    char, h = predict_next_char(model, char, h)    chars. append(char)  return prime + ''. join([test_dataset. itos[x] for x in chars])Notice that in the predict function, we are not simply taking softmax and using the character with the highest probability as the output. This is to ensure non deterministic behaviour of our model. Otherwise it is possible that our model will predict the same text every single time given the same input. To avoid this, we add a bit of randomness to the predictions by using a categorical distribution from torch. distributions. Categorical. We map our output probabilities and sample from this distribution hence making it slightly random. Another important point is that we have to call model. eval() before making any predictions. This is to ensure that factors like dropout are not active during inference. Here is what the output looks like: 1234567891011121314151617181920212223242526272829303132Thenoud. That may advents and then were what he had realized him to with it his word really he had hoped to gravely. The Girls were todirectly down as Moscow  and many name in Frenchsiatures: still triedto goto the man and took peasants with recembling officer replied. Without expectated her.  To last after it of them; if he asked Doroth, do what a words she wrote out. Might Russia, and how he knew us to the princess. See Pierre close, I. When she saw the God, as he had khow. Napoleon's beauty-embingsheopposite if and down to cur, beamed his spoze, remained gazed at his long dipfushius fur rash,went in land. Thestaff so endarked her stage. There, evidently sat people, paper the Emperor respect and had a quarter appear mighterand huers from the twill freed large. Count Molming which was loved to very offended kind collar out, and herself that like Borodino unattended to Prince Vasili.  I must be done not become meeting his will, so. Ing a restraint from her resist up to a swerf and dispensation, Natasha, blostest gossip. Theseeverywhere he frew four same to their faces which drawn out himself, but Pierre never just always like her and heard I may cause the officers for that-sobling have meant not la in her producefre princess. On the Sonya, was understood here theyhad thought the princess, the yellow an one had understanding a moment--motionless a thousandsof a valand quarters. After the rubber word of how one interest less pulled on the bullet in the family. The Man. Petya spoke Pierrein his hearing an angry plundings. The hands of Pierre's resoluting thatwho would have so not havingoffended, sides tothe men meeting Miss Rostov, had a suture was employed at the brought, and that reformed remorsing and provising as particularly finely and funday the weep of side, while he was the entristent fiolly in a schabaces were ridicules surely-electations to the clowar!  he said that had beentaken intheRussian French hamp lagerral Rather--what was not repuThese are the first 2000 characters after training for 40 epochs. It is actually quite fun to see the model progress. It starts out printing random characters. Slowly but surely it starts to understand the structure of the corpus. Eventually you see it reduce the number of spelling mistakes. While the text doesn’t make any sense, let me put this in context. This is a model which has no understanding of the concept of language, let alone the english language. We have given it a pretty small corpus of data (with a tiny sequence length of 100) and all it has learnt is to predict the next character. I have been studying english for my entire life and I still make spelling msitakes every single day :p. I have trained this very simple and small model architecture for less than an hour and I must say, I’m pretty impressed with the results. If you want a link to the full code, you can find it on my github. If you have any doubts, concerns or suggestions, feel free to leave a comment. Have a nice day! "
    }, {
    "id": 7,
    "url": "http://localhost:4000/Neural-Style-Transfer/",
    "title": "Neural Style Transfer",
    "body": "2019/12/28 - &lt;!DOCTYPE html&gt;   "
    }, {
    "id": 8,
    "url": "http://localhost:4000/Content-Based-Image-Retrieval/",
    "title": "Content Based Image Retrieval (CBIR)",
    "body": "2019/12/28 - Simple image classification was a challenge in computer vision not so long ago. All of this changed with the use of deep CNN architectures. Models like ResNet that use skip connections, leading to much deeper architectures have consistently shown impressive results on the ImageNet dataset. Due to the success of these models in other tasks through transfer learning, it is apparent that they are able to extract relevant information from an RGB image. In this post, we will attempt to use a ResNet which has been trained on ImageNet to extract relevant features from our dataset and use these features to find similar images. This is broadly known as “Content Based Image Retrieval” where similar images are found based on semantic similarity. To replicate these results you will need PyTorch, faiss, NumPy and matplotlib. If you just want the code, I have it on my github. For this project, we will use this Jewellery dataset. This dataset contains four classes:  Bracelets (309 images).  Earrings (472 images).  Necklaces (301 images).  Rings (189 images). The images have the jewellery item in focus with a white background. This is a very clean dataset and should give us good results. Downloading the dataset: 1234wget --no-check-certificate 'https://docs. google. com/uc?export=download&amp;id=0B4KI-B-t3wTjbElMTS1DVldQUnc' -O Jewellery. tar. gztar xvf Jewellery. tar. gzrm Jewellery/*. gzrm Jewellery/*. zipThe above lines of code will download and extract the dataset using the terminal. They can be run in a jupyter notebook as well by inserting a ‘!’ before each line. After running the above code, you should have a directory called Jewellery which contains 4 different subdirectories with the names of each of the 4 different classes. Sound familiar? This is because this is exactly the format required by the torchvision. datasets. ImageFolder class! Unfortunately, as of the writing of this blog, this class does not return the name of the file. Building a custom ImageFolder class: We can make one very small modification to the builtin ImageFolder class so that it also returns the filenames. We require the file names to have a mapping of images with their extracted features. 123456789101112131415class ImageFolderWithPaths(datasets. ImageFolder):     Custom dataset that includes image file paths. Extends  torchvision. datasets. ImageFolder  Source: https://gist. github. com/andrewjong/6b02ff237533b3b2c554701fb53d5c4d         # override the __getitem__ method. this is the method that dataloader calls  def __getitem__(self, index):    # this is what ImageFolder normally returns     original_tuple = super(ImageFolderWithPaths, self). __getitem__(index)    # the image file path    path = self. imgs[index][0]    # make a new tuple that includes original and the path    tuple_with_path = (original_tuple + (path,))    return tuple_with_pathAs you can see from the above code, just by adding a couple of lines to the __getitem__ method, we are able to return the file names along with the image tensor and a label if necessary. Preprocessing the input data: We do not require a lot of preprocessing for this sample dataset. Here we will just resize the input images to (224 x 224) as that is the input size required by the ResNet. This can be achieved using a simple torchvision. transforms. Resize(). We also have to normalize our input tensor with the same parameters as used to train the network on imagenet. This is what our preprocessing looks like: 123456789transforms_ = transforms. Compose([  transforms. Resize(size=[224, 224], interpolation=2),  transforms. ToTensor(),  transforms. Normalize(mean=[0. 485, 0. 456, 0. 406],            std=[0. 229, 0. 224, 0. 225])])dataset = ImageFolderWithPaths('Jewellery', transforms_) # our custom datasetdataloader = torch. utils. data. DataLoader(dataset, batch_size=1)Downloading the model: We will be using the pretrained ResNet50 from torchvision. models. You can try using the same logic on multiple different CNN architectures but we will be using ResNet50 for this blog. 12DEVICE = 'cuda' if torch. cuda. is_available() else 'cpu'model = models. resnet50(pretrained=True)ResNet is by default used for classification. We don’t want the output from the output layer of the ResNet. We will consider our feature vector to be the output of the last pooling layer. To extract the output from this pooling layer, we will use a small function: 1234567def pooling_output(x):  global model  for layer_name, layer in model. _modules. items():    x = layer(x)    if layer_name == 'avgpool':      break  return xHere avgpool is the name of the last pooling layer in the structure of our model. Creating feature vectors: We now have everything we require to create our feature vectors. This is a very straightforward process. Make sure you put the model in eval() mode before running this! 1234567891011# iterate over dataimage_paths = []descriptors = []model. to(DEVICE)with torch. no_grad():  model. eval()  for inputs, labels, paths in dataloader:    result = pooling_output(inputs. to(DEVICE))    descriptors. append(result. cpu(). view(1, -1). numpy())    image_paths. append(paths)    torch. cuda. empty_cache()Once this code finishes execution, congratulations! You have now built feature vectors from your dataset. But how do you find similar images from these feature vectors? This is where faiss comes in. The description of faiss from its github is “A library for efficient similarity search and clustering of dense vectors”. This is a library created by Facebook which is super fast at similarity search, which is exactly what we want. Installing faiss: 1234wget https://anaconda. org/pytorch/faiss-gpu/1. 2. 1/download/linux-64/faiss-gpu-1. 2. 1-py36_cuda9. 0. 176_1. tar. bz2tar xvjf faiss-gpu-1. 2. 1-py36_cuda9. 0. 176_1. tar. bz2cp -r lib/python3. 6/site-packages/* /usr/local/lib/python3. 6/dist-packages/pip install mklYou may want to replace my version with the latest one. But I cannot promise that it will work the same, so in case of any errors, try installing the same version of faiss that I have. Creating a faiss index: The way that we will use faiss is that first we will create a faiss index using our precalculated feature vectors. Then at runtime we will get another image. We will then run this image through our model and calculate its feature vector as well. We will then query faiss with the new feature vector to find similar vectors. It should be clearer with code. 1234567import numpy as npimport faissindex = faiss. IndexFlatL2(2048)descriptors = np. vstack(descriptors)index. add(descriptors)Calculating the feature vector of a query image and searching using faiss: 12345678query_image = 'Jewellery/bracelet/bracelet_048. jpg'img = Image. open(query_image)input_tensor = transforms_(img)input_tensor = input_tensor. view(1, *input_tensor. shape)with torch. no_grad():  query_descriptors = pooling_output(input_tensor. to(DEVICE)). cpu(). numpy()  distance, indices = index. search(query_descriptors. reshape(1, 2048), 9)Using the above piece of code, I got the following results:    Query image:     Top 9 results:  The results are not that bad! The first image is just the query image as naturally it will have the most similar vector. The rest of the images are what I would say pretty similar to the query image. This is especially apparent because of the circular piece of jewellery at the center of the bracelet. But I would say for a model not trained at all on this specific dataset, the results are acceptable. You can try training the model on the actual dataset, augmenting the images, adding a bit of noise to make the model a bit more general or any other technique you want to try and improve the performance of the model. The complete code for this project is available in the form of a jupyter notebook on my github or on nbviewer. You can leave any questions, comments or concerns in the comment section below. I hope this post was useful :) "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});