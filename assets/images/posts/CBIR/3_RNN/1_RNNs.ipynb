{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have looked at Linear Neural Networks and Convolutional Neural Networks. So what is the need of Recurrent Neural Networks? There is an obvious limitation to the architectures which we have studied until now. Can you guess what it is?<br>\n",
    ".<br>\n",
    ".<br>\n",
    ".<br>\n",
    ".<br>\n",
    "Memory!<br>\n",
    "These architectures don't store any information about the previous inputs given to the network. This mean they tend to give poor results while working with sequential data (for the most part). Humans don't start thinking from scratch at every instant. Just while reading this sentence, you have an idea of the words which came before and the ones to follow. A Linear model processes each input independently. So you must convert the entire sequence into one input data point. They are hence stateless. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RNN is an architecture which unlike Linear models, preserve state. They process sequences by iterating through its elements and maintaining a <b>state</b>. This state is reset while processing two different sequences. This is what a simple RNN looks like:\n",
    "\n",
    "<img src='Images/RNN.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved state is called the <b>hidden state</b>. An RNN processes each element of the sequence sequentially. At each time step, it updates its hidden state and produces an output. This is what happens when we \"unroll\" an RNN:\n",
    "    \n",
    "<img src='Images/RNN_unrolled.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unrolling an RNN is simply visualizing how it processes the sequence element by element. In reality, the RNN consists of just one cell processing the input in a loop. This property of an RNN allows it to process variable length inputs. RNNs are just a refactored, fully-connected neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The working of an RNN (at timestep $t$) is as follows:\n",
    "An RNN consists of 3 weight matrices: $W_x$, $W_h$, $W_y$.\n",
    "- $W_x$ is the weight matrix for the input (x).\n",
    "- $W_h$ is the weight matrix for the hidden state.\n",
    "- $W_y$ is the weight matrix for the output.\n",
    "\n",
    "The hidden state is given by:<br>\n",
    "$H_t = \\sigma(W_x * X_t + W_h * H_{t-1})$\n",
    "- $H_t$ is the hidden state at timestep $t$.\n",
    "- $\\sigma$ is the activation function (generally sigmoid or tanh).\n",
    "- $X_t$ is the input at the current timestep.\n",
    "\n",
    "The output of the RNN is given by:<br>\n",
    "$y = \\sigma_y(W_y * H_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the sequential nature of natural language, RNNs are commonly used in Natural Language Processing. \n",
    "Let us try to better understand the working of RNNs using an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are going to build a model to classify names into two countries of origin -> Italy and Germany. Our dataset consists of two files `Italian.txt` and `German.txt`. Both of these files contain a single name on each line. The files follow the following format:\n",
    "```\n",
    "name_1\n",
    "name_2\n",
    "name_3\n",
    "...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pprint import pprint\n",
    "import os\n",
    "from string import ascii_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Projects/NameClassifier/data/names/German.txt', 'r') as german_f, open('Projects/NameClassifier/data/names/Italian.txt', 'r') as italian_f:\n",
    "    german_names = german_f.read()\n",
    "    italian_names = italian_f.read()\n",
    "\n",
    "print(f'German names:\\n{german_names[:30]}')\n",
    "print()\n",
    "print(f'Italian names:\\n{italian_names[:33]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all the unique characters in the files\n",
    "\n",
    "The classifier which we are going to build is going to be character based. This means that it will take a sequence of characters as its input. Each name will be read by the model character by character. For this we need to first find all the unique characters in the files. We find all the unique characters in the files and then take its union with all letters (uppercase and lowercase) to form our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzßàäèéìòóöùü\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_characters = set((german_names + italian_names).replace('\\n', '')).union(set(ascii_letters))\n",
    "unique_characters = list(unique_characters)\n",
    "''.join(sorted(unique_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abbing', 'Abel', 'Abeln', 'Abt', 'Achilles']\n",
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "german_names = german_names.split('\\n')\n",
    "italian_names = italian_names.split('\\n')\n",
    "print(german_names[:5])\n",
    "print(italian_names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing common names\n",
    "\n",
    "We don't want our classifier to accept the same input with two different classes. Hence, we will find the names which exist in both the german and italian datasets and remove them from both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'Salomon', 'Paternoster']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_names = list(set(german_names).intersection(set(italian_names)))\n",
    "common_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for common_name in common_names:\n",
    "    german_names.remove(common_name)\n",
    "    italian_names.remove(common_name)\n",
    "    \n",
    "common_names = list(set(german_names).intersection(set(italian_names)))\n",
    "common_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our data\n",
    "\n",
    "We will create a list of all our names. This will be the input passed to our model. Along with this we will also need labels. We will have a label of `0` for german names and a label of `1` for italian names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zimmerman', 'Zimmermann', 'Abandonato', 'Abatangelo', 'Abatantuono', 'Abate']\n",
      "[0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "german_label = [0]\n",
    "italian_label = [1]\n",
    "\n",
    "all_names = german_names + italian_names\n",
    "all_labels = german_label * len(german_names) + italian_label * len(italian_names)\n",
    "print(all_names[720:726])\n",
    "print(all_labels[720:726])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding characters\n",
    "\n",
    "For our model to be able to process our input, we have to convert the characters to one hot encoded vectors. The size of our vector will be the total number of unique characters in our dataset. Hence we will first create a mapping of our character and its index. We can then use this mapping to convert our input characters to digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " \"'\": 1,\n",
       " 'A': 2,\n",
       " 'B': 3,\n",
       " 'C': 4,\n",
       " 'D': 5,\n",
       " 'E': 6,\n",
       " 'F': 7,\n",
       " 'G': 8,\n",
       " 'H': 9,\n",
       " 'I': 10,\n",
       " 'J': 11,\n",
       " 'K': 12,\n",
       " 'L': 13,\n",
       " 'M': 14,\n",
       " 'N': 15,\n",
       " 'O': 16,\n",
       " 'P': 17,\n",
       " 'Q': 18,\n",
       " 'R': 19,\n",
       " 'S': 20,\n",
       " 'T': 21,\n",
       " 'U': 22,\n",
       " 'V': 23,\n",
       " 'W': 24,\n",
       " 'X': 25,\n",
       " 'Y': 26,\n",
       " 'Z': 27,\n",
       " 'a': 28,\n",
       " 'b': 29,\n",
       " 'c': 30,\n",
       " 'd': 31,\n",
       " 'e': 32,\n",
       " 'f': 33,\n",
       " 'g': 34,\n",
       " 'h': 35,\n",
       " 'i': 36,\n",
       " 'j': 37,\n",
       " 'k': 38,\n",
       " 'l': 39,\n",
       " 'm': 40,\n",
       " 'n': 41,\n",
       " 'o': 42,\n",
       " 'p': 43,\n",
       " 'q': 44,\n",
       " 'r': 45,\n",
       " 's': 46,\n",
       " 't': 47,\n",
       " 'u': 48,\n",
       " 'v': 49,\n",
       " 'w': 50,\n",
       " 'x': 51,\n",
       " 'y': 52,\n",
       " 'z': 53,\n",
       " 'ß': 54,\n",
       " 'à': 55,\n",
       " 'ä': 56,\n",
       " 'è': 57,\n",
       " 'é': 58,\n",
       " 'ì': 59,\n",
       " 'ò': 60,\n",
       " 'ó': 61,\n",
       " 'ö': 62,\n",
       " 'ù': 63,\n",
       " 'ü': 64}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi =  {char:idx for idx, char in enumerate(sorted(unique_characters))}\n",
    "stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our RNN can accept inputs of variable length, we still have to define a sequence length. This will allow us to batch our data for parallel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of stoi: 65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encoder(name, sequence_length):\n",
    "    global stoi\n",
    "    size = len(stoi)\n",
    "    print(f'Size of stoi: {size}')\n",
    "    # To save output\n",
    "    encoded = []\n",
    "    # Iterating through name\n",
    "    for char in name:\n",
    "        temp = torch.zeros(size)\n",
    "        # Setting index of character to 1\n",
    "        temp[stoi[char]] = 1\n",
    "        encoded.append(temp)\n",
    "        \n",
    "    # Filling the rest of the sequence with zeros\n",
    "    for i in range(sequence_length - len(name)):\n",
    "        temp = torch.zeros(size)\n",
    "        encoded.append(temp)\n",
    "\n",
    "    return torch.stack(encoded)\n",
    "\n",
    "one_hot_encoder('Aniket', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our dataset object\n",
    "\n",
    "Now we have done a lot of preprocessing! Let us combine all of this in our dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, german_fname='Projects/NameClassifier/data/names/German.txt', italian_fname='Projects/NameClassifier/data/names/Italian.txt'):\n",
    "        super().__init__()\n",
    "        # Reading from files\n",
    "        with open(german_fname, 'r') as german_f, open(italian_fname, 'r') as italian_f:\n",
    "            german_names = german_f.read()\n",
    "            italian_names = italian_f.read()\n",
    "        \n",
    "        # Finding unique characters\n",
    "        unique_characters = list(set((german_names + italian_names).replace('\\n', '')).union(set(ascii_letters)))\n",
    "        german_names = german_names.split('\\n')\n",
    "        italian_names = italian_names.split('\\n')\n",
    "        \n",
    "        # Removing common names\n",
    "        common_names = list(set(german_names).intersection(set(italian_names)))\n",
    "        for common_name in common_names:\n",
    "            german_names.remove(common_name)\n",
    "            italian_names.remove(common_name)\n",
    "        german_label = [0]\n",
    "        italian_label = [1]\n",
    "\n",
    "        # Setting names and labels\n",
    "        self.names = german_names + italian_names\n",
    "        self.labels = german_label * len(german_names) + italian_label * len(italian_names)\n",
    "        \n",
    "        # Mapping from chars to int\n",
    "        self.stoi =  {char:idx for idx, char in enumerate(sorted(unique_characters))}\n",
    "        \n",
    "        # Size of longest word is 18\n",
    "        self.sequence_length = 18\n",
    "        \n",
    "        # One hot encoded names\n",
    "        self.encoded_names = self.encode_dataset()\n",
    "\n",
    "    def one_hot_encoder(self, name):\n",
    "        size = len(self.stoi)\n",
    "\n",
    "        encoded = []\n",
    "        for char in name:\n",
    "            temp = torch.zeros(size)\n",
    "            temp[self.stoi[char]] = 1\n",
    "            encoded.append(temp)\n",
    "\n",
    "        for i in range(self.sequence_length - len(name)):\n",
    "            temp = torch.zeros(size)\n",
    "            encoded.append(temp)\n",
    "\n",
    "        return torch.stack(encoded)\n",
    "        \n",
    "    def encode_dataset(self):\n",
    "        encoded_list = []\n",
    "        for name in self.names:\n",
    "            encoded_list.append(self.one_hot_encoder(name))\n",
    "            \n",
    "        return torch.stack(encoded_list)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_names[idx], torch.tensor([self.labels[idx]])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([0]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = NameDataset()\n",
    "names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 65])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of input tensor (one word)\n",
    "names[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "split_ratio = 0.8\n",
    "data_len = len(names)\n",
    "train_size = int(split_ratio * data_len)\n",
    "test_size = data_len - train_size\n",
    "\n",
    "# Randomly splits data into given sizes\n",
    "train_dataset, test_dataset = random_split(names, lengths=(train_size, test_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with a Linear model\n",
    "\n",
    "Before we build our RNN based model, let us look at the results from a conventional Linear model which you have studied until now. On our problem statement, using a 3 layer linear model, I was able to achieve an accuracy of just 69.2% even after training for 100 epochs. This problem is very simple with very short sequences. For a larger problem with longer sequences, the model's performance would be even worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an RNN using linear layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us revisit the mathematics behind an RNN:\n",
    "$H_t = \\sigma(W_x * X_t + W_h * H_{t-1})$\n",
    "$y = \\sigma_y(W_y * H_t)$\n",
    "\n",
    "Where $H_t$ is the hidden state at timestep $t$ and $y$ is the output of the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        global device\n",
    "        self.device = device\n",
    "        self.hidden_size = 256\n",
    "        self.sequence_length = 18\n",
    "        self.input_size = 65\n",
    "        self.Wx = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.Wh = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.Wy = nn.Linear(self.sequence_length * self.hidden_size, self.hidden_size)\n",
    "        self.h = torch.zeros(1, self.hidden_size).to(self.device)\n",
    "        self.output_layer = nn.Linear(self.hidden_size, 2)\n",
    "        self.bn = nn.BatchNorm1d(self.hidden_size)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "#         h = self.h\n",
    "        h = torch.zeros(1, self.hidden_size).to(self.device)\n",
    "        res = []\n",
    "        for i in range(input_tensor.shape[1]):     # input_tensor.shape[1] = sequence length\n",
    "            x = F.tanh(self.Wx(input_tensor[:, i]))     # input_tensor[:, i] = the ith element in the sequence\n",
    "            h = F.tanh(self.Wh(h))\n",
    "            h = torch.add(h, x)\n",
    "            res.append(h)\n",
    "        \n",
    "        self.h = h.detach()        \n",
    "        res = torch.stack(res, dim=1)\n",
    "        res = res.reshape(-1, self.sequence_length * self.hidden_size)\n",
    "        res = F.relu(self.Wy(res))\n",
    "        res = F.softmax(self.output_layer(res))\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "linear_train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "linear_test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing model found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRNN(\n",
       "  (Wx): Linear(in_features=65, out_features=256, bias=True)\n",
       "  (Wh): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (Wy): Linear(in_features=4608, out_features=256, bias=True)\n",
       "  (output_layer): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "max_accuracy = 0.0\n",
    "MODEL_PATH = ''\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print('Existing model found!')\n",
    "    load_weights(model, MODEL_PATH)\n",
    "else:\n",
    "    print('No existing model found.')\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.85476815398076\n",
      "Loss: 0.6853206089892516\n",
      "Epoch: 1\n",
      "Accuracy: 75.32808398950131\n",
      "Loss: 0.6586288675235639\n",
      "Epoch: 2\n",
      "Accuracy: 82.76465441819772\n",
      "Loss: 0.6005860694854382\n",
      "### TESTING ###\n",
      "        Accuracy: 82.87\n",
      "        Loss: 0.5592\n",
      "Epoch: 3\n",
      "Accuracy: 81.97725284339458\n",
      "Loss: 0.5180254277177578\n",
      "Epoch: 4\n",
      "Accuracy: 83.46456692913385\n",
      "Loss: 0.48068066542334237\n",
      "Epoch: 5\n",
      "Accuracy: 86.3517060367454\n",
      "Loss: 0.45952158613572075\n",
      "### TESTING ###\n",
      "        Accuracy: 84.61999999999999\n",
      "        Loss: 0.4619\n",
      "Epoch: 6\n",
      "Accuracy: 87.83902012248468\n",
      "Loss: 0.4428591350662218\n",
      "Epoch: 7\n",
      "Accuracy: 89.501312335958\n",
      "Loss: 0.42897117972269666\n",
      "Epoch: 8\n",
      "Accuracy: 90.5511811023622\n",
      "Loss: 0.41748434638935333\n",
      "### TESTING ###\n",
      "        Accuracy: 89.51\n",
      "        Loss: 0.4273\n",
      "Epoch: 9\n",
      "Accuracy: 91.42607174103236\n",
      "Loss: 0.40820089110343577\n",
      "Epoch: 10\n",
      "Accuracy: 91.68853893263342\n",
      "Loss: 0.4006210315258797\n",
      "Epoch: 11\n",
      "Accuracy: 92.56342957130359\n",
      "Loss: 0.39420335441958143\n",
      "### TESTING ###\n",
      "        Accuracy: 91.25999999999999\n",
      "        Loss: 0.4129\n",
      "Epoch: 12\n",
      "Accuracy: 93.1758530183727\n",
      "Loss: 0.38870117540226\n",
      "Epoch: 13\n",
      "Accuracy: 93.61329833770779\n",
      "Loss: 0.3841274519917846\n",
      "Epoch: 14\n",
      "Accuracy: 94.05074365704287\n",
      "Loss: 0.3804997231808026\n",
      "### TESTING ###\n",
      "        Accuracy: 90.56\n",
      "        Loss: 0.4073\n",
      "Epoch: 15\n",
      "Accuracy: 94.40069991251093\n",
      "Loss: 0.3776694426177785\n",
      "Epoch: 16\n",
      "Accuracy: 94.40069991251093\n",
      "Loss: 0.3754057561944476\n",
      "Epoch: 17\n",
      "Accuracy: 94.57567804024497\n",
      "Loss: 0.37349433718495273\n",
      "### TESTING ###\n",
      "        Accuracy: 90.91\n",
      "        Loss: 0.4039\n",
      "Epoch: 18\n",
      "Accuracy: 94.66316710411199\n",
      "Loss: 0.3718259826837041\n",
      "Epoch: 19\n",
      "Accuracy: 94.83814523184601\n",
      "Loss: 0.3703553803323761\n",
      "Epoch: 20\n",
      "Accuracy: 94.92563429571304\n",
      "Loss: 0.36905212852898545\n",
      "### TESTING ###\n",
      "        Accuracy: 90.91\n",
      "        Loss: 0.402\n",
      "Epoch: 21\n",
      "Accuracy: 94.750656167979\n",
      "Loss: 0.36788002879392234\n",
      "Epoch: 22\n",
      "Accuracy: 94.66316710411199\n",
      "Loss: 0.3667958560786744\n",
      "Epoch: 23\n",
      "Accuracy: 94.750656167979\n",
      "Loss: 0.36577873806732325\n",
      "### TESTING ###\n",
      "        Accuracy: 91.61\n",
      "        Loss: 0.4001\n",
      "Epoch: 24\n",
      "Accuracy: 94.92563429571304\n",
      "Loss: 0.3648265244558206\n",
      "Epoch: 25\n",
      "Accuracy: 94.92563429571304\n",
      "Loss: 0.36392717155988863\n",
      "Epoch: 26\n",
      "Accuracy: 95.10061242344707\n",
      "Loss: 0.36307102937189406\n",
      "### TESTING ###\n",
      "        Accuracy: 91.61\n",
      "        Loss: 0.3973\n",
      "Epoch: 27\n",
      "Accuracy: 95.18810148731409\n",
      "Loss: 0.3622470853209808\n",
      "Epoch: 28\n",
      "Accuracy: 95.2755905511811\n",
      "Loss: 0.3614634818813828\n",
      "Epoch: 29\n",
      "Accuracy: 95.53805774278216\n",
      "Loss: 0.3607256871925981\n",
      "### TESTING ###\n",
      "        Accuracy: 92.31\n",
      "        Loss: 0.394\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    print(f'Epoch: {epoch}')\n",
    "    for input_batch, labels in linear_train_loader:\n",
    "        if labels.size(0) != batch_size: continue\n",
    "        model.zero_grad()\n",
    "#         print(input_batch.shape)\n",
    "        output = model.forward(input_batch.to(device))\n",
    "#         print(output)\n",
    "#         print(labels)\n",
    "#         \n",
    "#         print(output.shape, labels.shape)\n",
    "        loss = criterion(output, labels.to(device).long().reshape(batch_size,))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        total += labels.size(0)\n",
    "        correct += torch.sum(torch.argmax(output, dim=1).view(1, -1) == labels.to(device).view(1, -1)).item()\n",
    "#         break\n",
    "#     break\n",
    "    \n",
    "    \n",
    "    print(f'Accuracy: {correct/total * 100}\\nLoss: {epoch_loss/total}')\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        test_epoch_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.eval()\n",
    "        for input_batch, labels in linear_test_loader:\n",
    "            with torch.no_grad():\n",
    "                if labels.size(0) != batch_size: continue\n",
    "                output = model.forward(input_batch.to(device))\n",
    "                loss = criterion(output, labels.to(device).long().reshape(batch_size,))\n",
    "                test_epoch_loss += loss.item()\n",
    "                total += labels.size(0)\n",
    "                correct += torch.sum(torch.argmax(output, dim=1).view(1, -1) == labels.to(device).view(1, -1)).item()\n",
    "#                 print(torch.argmax(output, dim=0))\n",
    "#                 print(labels)\n",
    "#                 break\n",
    "    \n",
    "        test_accuracy = round(correct/total, 4) * 100\n",
    "        print(f'''### TESTING ###\n",
    "        Accuracy: {test_accuracy}\n",
    "        Loss: {round(test_epoch_loss/total, 4)}''')\n",
    "#         if max_accuracy < test_accuracy:\n",
    "#             max_accuracy = test_accuracy\n",
    "#             save_weights(model, MODEL_PATH)\n",
    "#             print('Best model found!')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with just 30 epochs of training we are able to achieve testing accuracy of more than <b>92%</b>. The RNN implementation we saw above is just to provide insight into the working of RNNs. I don't recommend anyone to actually build their own RNNs while working on their projects. We will now use the `torch.nn.RNN` module on the same problem. This is a much faster implementation which supports parallel processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameClassifier(nn.Module):\n",
    "    def __init__(self, max_len=18, hidden_size=256, input_size=65):\n",
    "        super().__init__()\n",
    "        dropout_prob = 0.4\n",
    "        self.input_size = input_size\n",
    "        self.sequence_length = max_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 2\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size, \n",
    "            num_layers=self.num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob\n",
    "        )\n",
    "        self.dropout_layer = nn.Dropout(dropout_prob)\n",
    "        self.linear_layer = nn.Linear(self.hidden_size * self.sequence_length, 256)\n",
    "        self.output_layer = nn.Linear(256, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_tensor, hidden):\n",
    "        rnn_output, new_hidden = self.rnn(input_tensor, hidden)\n",
    "        rnn_output = self.dropout_layer(rnn_output)\n",
    "#         print(f'RNN output: {rnn_output.size()}')\n",
    "#         linear_input = rnn_output[:, -1]\n",
    "#         print(f'Linear input: {linear_input.size()}')\n",
    "#         linear_output = F.relu(self.linear_layer(linear_input.reshape(-1, self.hidden_size * )))\n",
    "        linear_output = F.relu(self.linear_layer(rnn_output.reshape(-1, self.hidden_size * self.sequence_length)))\n",
    "        output = F.softmax(self.output_layer(linear_output))\n",
    "        new_hidden = new_hidden.detach()\n",
    "        return output, new_hidden\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NameClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NameClassifier(\n",
       "  (rnn): RNN(65, 256, num_layers=2, batch_first=True, dropout=0.4)\n",
       "  (dropout_layer): Dropout(p=0.4)\n",
       "  (linear_layer): Linear(in_features=4608, out_features=256, bias=True)\n",
       "  (output_layer): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "max_accuracy = 0.0\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.58450704225353\n",
      "Loss: 0.086549880848804\n",
      "Epoch: 1\n",
      "Accuracy: 51.6725352112676\n",
      "Loss: 0.08647492449258415\n",
      "Epoch: 2\n",
      "Accuracy: 51.76056338028169\n",
      "Loss: 0.08638013862598111\n",
      "### TESTING ###\n",
      "        Accuracy: 48.209999999999994\n",
      "        Loss: 0.0864\n",
      "Best model found!\n",
      "Epoch: 3\n",
      "Accuracy: 52.55281690140845\n",
      "Loss: 0.08620173363408572\n",
      "Epoch: 4\n",
      "Accuracy: 55.1056338028169\n",
      "Loss: 0.08603270105283026\n",
      "Epoch: 5\n",
      "Accuracy: 58.71478873239436\n",
      "Loss: 0.08576032760697351\n",
      "### TESTING ###\n",
      "        Accuracy: 60.36\n",
      "        Loss: 0.0855\n",
      "Best model found!\n",
      "Epoch: 6\n",
      "Accuracy: 62.676056338028175\n",
      "Loss: 0.08538978919386864\n",
      "Epoch: 7\n",
      "Accuracy: 69.54225352112677\n",
      "Loss: 0.08472762912721701\n",
      "Epoch: 8\n",
      "Accuracy: 76.76056338028168\n",
      "Loss: 0.0831931986859147\n",
      "### TESTING ###\n",
      "        Accuracy: 78.93\n",
      "        Loss: 0.081\n",
      "Best model found!\n",
      "Epoch: 9\n",
      "Accuracy: 78.4330985915493\n",
      "Loss: 0.07857666133155286\n",
      "Epoch: 10\n",
      "Accuracy: 78.4330985915493\n",
      "Loss: 0.07003735299681274\n",
      "Epoch: 11\n",
      "Accuracy: 80.36971830985915\n",
      "Loss: 0.06540740795538459\n",
      "### TESTING ###\n",
      "        Accuracy: 83.93\n",
      "        Loss: 0.0602\n",
      "Best model found!\n",
      "Epoch: 12\n",
      "Accuracy: 82.04225352112677\n",
      "Loss: 0.061705095562296856\n",
      "Epoch: 13\n",
      "Accuracy: 84.24295774647888\n",
      "Loss: 0.05905804966746921\n",
      "Epoch: 14\n",
      "Accuracy: 85.2112676056338\n",
      "Loss: 0.05736294611763786\n",
      "### TESTING ###\n",
      "        Accuracy: 88.92999999999999\n",
      "        Loss: 0.0534\n",
      "Best model found!\n",
      "Epoch: 15\n",
      "Accuracy: 86.17957746478874\n",
      "Loss: 0.05589043941925949\n",
      "Epoch: 16\n",
      "Accuracy: 88.20422535211267\n",
      "Loss: 0.05467819627112066\n",
      "Epoch: 17\n",
      "Accuracy: 88.02816901408451\n",
      "Loss: 0.053680163060485474\n",
      "### TESTING ###\n",
      "        Accuracy: 90.0\n",
      "        Loss: 0.051\n",
      "Best model found!\n",
      "Epoch: 18\n",
      "Accuracy: 88.99647887323944\n",
      "Loss: 0.05291073205290546\n",
      "Epoch: 19\n",
      "Accuracy: 89.87676056338029\n",
      "Loss: 0.052118927869998236\n",
      "Epoch: 20\n",
      "Accuracy: 90.05281690140845\n",
      "Loss: 0.05185337506339584\n",
      "### TESTING ###\n",
      "        Accuracy: 91.79\n",
      "        Loss: 0.0495\n",
      "Best model found!\n",
      "Epoch: 21\n",
      "Accuracy: 90.84507042253522\n",
      "Loss: 0.05125128073801457\n",
      "Epoch: 22\n",
      "Accuracy: 91.28521126760563\n",
      "Loss: 0.05055123512488856\n",
      "Epoch: 23\n",
      "Accuracy: 91.81338028169014\n",
      "Loss: 0.05025755115587947\n",
      "### TESTING ###\n",
      "        Accuracy: 92.5\n",
      "        Loss: 0.0483\n",
      "Best model found!\n",
      "Epoch: 24\n",
      "Accuracy: 91.72535211267606\n",
      "Loss: 0.050039837346740175\n",
      "Epoch: 25\n",
      "Accuracy: 91.98943661971832\n",
      "Loss: 0.049815898971028734\n",
      "Epoch: 26\n",
      "Accuracy: 92.0774647887324\n",
      "Loss: 0.049665523325683365\n",
      "### TESTING ###\n",
      "        Accuracy: 92.14\n",
      "        Loss: 0.048\n",
      "Epoch: 27\n",
      "Accuracy: 91.90140845070422\n",
      "Loss: 0.049783599733466834\n",
      "Epoch: 28\n",
      "Accuracy: 92.6056338028169\n",
      "Loss: 0.04917106074346623\n",
      "Epoch: 29\n",
      "Accuracy: 92.42957746478874\n",
      "Loss: 0.04905853562161956\n",
      "### TESTING ###\n",
      "        Accuracy: 93.57\n",
      "        Loss: 0.0475\n",
      "Best model found!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    hidden = model.init_hidden(batch_size=batch_size)\n",
    "    print(f'Epoch: {epoch}')\n",
    "    for input_batch, labels in train_loader:\n",
    "        if labels.size(0) != batch_size: continue\n",
    "        model.zero_grad()\n",
    "        output, hidden = model.forward(input_batch.to(device), hidden.to(device))\n",
    "#         print(output)\n",
    "#         print(labels)\n",
    "#         \n",
    "        loss = criterion(output, labels.to(device).long().reshape(batch_size,))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        total += labels.size(0)\n",
    "        correct += torch.sum(torch.argmax(output, dim=1).view(1, -1) == labels.to(device).view(1, -1)).item()\n",
    "    \n",
    "    print(f'Accuracy: {correct/total * 100}\\nLoss: {epoch_loss/total}')\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        test_epoch_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.eval()\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        for input_batch, labels in test_loader:\n",
    "            with torch.no_grad():\n",
    "                if labels.size(0) != batch_size: continue\n",
    "                output, hidden = model.forward(input_batch.to(device), hidden.to(device))\n",
    "                loss = criterion(output, labels.to(device).long().reshape(batch_size,))\n",
    "                test_epoch_loss += loss.item()\n",
    "                total += labels.size(0)\n",
    "                correct += torch.sum(torch.argmax(output, dim=1).view(1, -1) == labels.to(device).view(1, -1)).item()\n",
    "#                 print(torch.argmax(output, dim=0))\n",
    "#                 print(labels)\n",
    "#                 break\n",
    "                \n",
    "        test_accuracy = round(correct/total, 4) * 100\n",
    "        print(f'''### TESTING ###\n",
    "        Accuracy: {test_accuracy}\n",
    "        Loss: {round(test_epoch_loss/total, 4)}''')\n",
    "        if max_accuracy < test_accuracy:\n",
    "            max_accuracy = test_accuracy\n",
    "#             save_weights(model, MODEL_PATH)\n",
    "            print('Best model found!')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we get very similar accuracies from the two models. This is because they are essentially doing the same thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model with user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian\n",
      "Confidence: 0.9999949932098389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input_tensor = names.one_hot_encoder('Tribbiani')\n",
    "    input_tensor = input_tensor.view(1, *input_tensor.shape)\n",
    "    output = model.forward(input_tensor.to(device), hidden.to(device))\n",
    "    class_ = torch.argmax(output[0], dim=1).item()\n",
    "    print('German' if class_ == 0 else 'Italian')\n",
    "    print(f'Confidence: {output[0][0][class_]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
